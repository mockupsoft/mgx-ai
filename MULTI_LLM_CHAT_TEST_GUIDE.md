# Multi-LLM ve CanlÄ± Sohbet Test Rehberi

## ğŸš€ HÄ±zlÄ± Test

### 1. Multi-LLM YÃ¶netimi SayfasÄ±
**URL**: http://localhost:3000/mgx/settings/llm

**Test AdÄ±mlarÄ±**:
1. TarayÄ±cÄ±da sayfayÄ± aÃ§Ä±n
2. Provider Health bÃ¶lÃ¼mÃ¼nde tÃ¼m provider'larÄ±n durumunu gÃ¶rÃ¼n:
   - âœ… Anthropic (4 model)
   - âœ… Mistral (4 model)
   - âœ… Ollama (6 model - local)
   - âœ… OpenAI (5 model)
   - âœ… Together AI (3 model)
3. Routing Strategy seÃ§in (balanced, cost_optimized, latency_optimized, quality_optimized, local_first)
4. "Test Route" butonuna tÄ±klayÄ±n
5. SeÃ§ilen provider ve model'i gÃ¶rÃ¼n

### 2. CanlÄ± Sohbet AkÄ±ÅŸÄ±
**URL**: http://localhost:3000/mgx/tasks

**Test AdÄ±mlarÄ±**:
1. Tasks sayfasÄ±na gidin
2. Bir task seÃ§in veya yeni task oluÅŸturun
3. Task monitoring view'de canlÄ± sohbet paneli gÃ¶rÃ¼nÃ¼r
4. WebSocket baÄŸlantÄ±sÄ± otomatik kurulur
5. Agent mesajlarÄ± gerÃ§ek zamanlÄ± olarak gÃ¶rÃ¼nÃ¼r

## ğŸ”§ Multi-LLM NasÄ±l Ã‡alÄ±ÅŸÄ±yor?

### Backend'de Multi-LLM
1. **Team Config**: `use_multi_llm=True` olduÄŸunda her role farklÄ± LLM kullanÄ±r
2. **LLM Router**: Stratejiye gÃ¶re provider seÃ§er (balanced, cost, latency, quality, local_first)
3. **Fallback Chain**: Bir provider baÅŸarÄ±sÄ±z olursa otomatik olarak diÄŸerine geÃ§er

### CanlÄ± Sohbet AkÄ±ÅŸÄ±
1. **WebSocket BaÄŸlantÄ±sÄ±**: `ws://localhost:8000/ws/tasks/{task_id}`
2. **Mesaj AkÄ±ÅŸÄ±**: 
   - Agent mesajlarÄ± WebSocket Ã¼zerinden gelir
   - Typing indicator gÃ¶sterilir
   - Mesajlar otomatik scroll edilir
3. **Mesaj Tipleri**:
   - `user` - KullanÄ±cÄ± mesajlarÄ±
   - `agent` - Agent mesajlarÄ±
   - `tool` - Tool execution mesajlarÄ±
   - `system` - Sistem mesajlarÄ±
   - `error` - Hata mesajlarÄ±

## ğŸ“Š API Endpoints

### LLM Management
```bash
# Provider health
GET http://localhost:8000/api/llm/health

# Route test
POST http://localhost:8000/api/llm/route
Body: {
  "strategy": "balanced",
  "prefer_local": false
}

# Model listesi
GET http://localhost:8000/api/llm/models?provider=openai
```

### Agent Messages
```bash
# Mesaj listesi
GET http://localhost:8000/api/agents/{agent_id}/messages

# Mesaj gÃ¶nderme
POST http://localhost:8000/api/agents/{agent_id}/messages
Body: {
  "content": "Merhaba",
  "direction": "outbound"
}
```

### WebSocket
```javascript
// Task events
ws://localhost:8000/ws/tasks/{task_id}

// Agent events
ws://localhost:8000/ws/agents/{agent_id}

// Global stream
ws://localhost:8000/ws/stream
```

## ğŸ¯ Test SenaryolarÄ±

### Senaryo 1: Multi-LLM Routing Test
1. LLM Management sayfasÄ±na gidin
2. Strategy'yi "cost_optimized" seÃ§in
3. "Test Route" butonuna tÄ±klayÄ±n
4. En ucuz provider/model seÃ§ildiÄŸini gÃ¶rÃ¼n

### Senaryo 2: CanlÄ± Sohbet Test
1. Bir task oluÅŸturun
2. Task'Ä± Ã§alÄ±ÅŸtÄ±rÄ±n
3. CanlÄ± sohbet panelinde agent mesajlarÄ±nÄ± gÃ¶rÃ¼n
4. WebSocket baÄŸlantÄ±sÄ±nÄ±n Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± doÄŸrulayÄ±n

### Senaryo 3: Multi-LLM + CanlÄ± Sohbet
1. Team config'de `use_multi_llm=True` ayarlayÄ±n
2. Bir task oluÅŸturun
3. Her role farklÄ± LLM kullanÄ±ldÄ±ÄŸÄ±nÄ± loglardan kontrol edin
4. CanlÄ± sohbet panelinde mesajlarÄ± gÃ¶rÃ¼n

## ğŸ” Debugging

### WebSocket BaÄŸlantÄ±sÄ± KontrolÃ¼
```javascript
// Browser console'da
const ws = new WebSocket('ws://localhost:8000/ws/stream');
ws.onopen = () => console.log('Connected');
ws.onmessage = (e) => console.log('Message:', JSON.parse(e.data));
```

### Provider Health KontrolÃ¼
```bash
curl http://localhost:8000/api/llm/health
```

### Route Test
```bash
curl -X POST http://localhost:8000/api/llm/route \
  -H "Content-Type: application/json" \
  -d '{"strategy":"balanced"}'
```

## ğŸ“ Notlar

- Multi-LLM modu aktif olduÄŸunda her role farklÄ± LLM kullanÄ±r
- CanlÄ± sohbet WebSocket ile Ã§alÄ±ÅŸÄ±r, gerÃ§ek zamanlÄ± mesaj akÄ±ÅŸÄ± saÄŸlar
- Routing stratejisi task'a gÃ¶re otomatik seÃ§ilir
- Fallback chain sayesinde provider hatalarÄ±nda otomatik geÃ§iÅŸ yapÄ±lÄ±r

