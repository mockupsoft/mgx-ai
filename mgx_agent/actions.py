# -*- coding: utf-8 -*-
"""
MGX Agent Actions Module

LLM Ã§aÄŸrÄ±larÄ± yapan Action sÄ±nÄ±flarÄ±:
- AnalyzeTask: GÃ¶rev karmaÅŸÄ±klÄ±k analizi
- DraftPlan: GÃ¶rev planÄ± taslaÄŸÄ±
- WriteCode: Kod yazma
- WriteTest: Test yazma
- ReviewCode: Kod inceleme
"""

import os
import re
from datetime import datetime, timezone
from typing import List, Tuple, Optional

from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
from metagpt.actions import Action
from metagpt.logs import logger

from mgx_observability import (
    ObservabilityConfig,
    get_langsmith_logger,
    record_exception,
    set_span_attributes,
    start_span,
)


def llm_retry():
    """LLM Ã§aÄŸrÄ±larÄ± iÃ§in retry decorator"""
    return retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        retry=retry_if_exception_type((Exception,)),
        before_sleep=lambda retry_state: logger.warning(
            f"âš ï¸ LLM hatasÄ±, yeniden deneniyor... (Deneme {retry_state.attempt_number}/3)"
        )
    )


def print_step_progress(step: int, total: int, description: str, role=None):
    """AdÄ±m adÄ±m progress gÃ¶ster
    
    Args:
        step: Mevcut adÄ±m
        total: Toplam adÄ±m
        description: AÃ§Ä±klama
        role: Role instance (team referansÄ± iÃ§in)
    """
    # EÄŸer role'un team referansÄ± varsa onu kullan (config kontrolÃ¼ iÃ§in)
    if role and hasattr(role, '_team_ref') and hasattr(role._team_ref, '_print_progress'):
        role._team_ref._print_progress(step, total, description)
        return
    
    # Fallback: Global fonksiyon (eski davranÄ±ÅŸ)
    bar_length = 20
    filled = int(bar_length * step / total)
    bar = "â–ˆ" * filled + "â–‘" * (bar_length - filled)
    percent = int(100 * step / total)
    print(f"\r[{bar}] {percent}% - {description}", end="", flush=True)
    if step == total:
        print()  # Yeni satÄ±r


def print_phase_header(phase: str, emoji: str = "ğŸ”„"):
    """Faz baÅŸlÄ±ÄŸÄ± yazdÄ±r"""
    print(f"\n{'='*60}")
    print(f"{emoji} {phase}")
    print(f"{'='*60}")


def _env_flag(name: str) -> bool:
    return str(os.getenv(name, "")).strip().lower() in {"1", "true", "yes", "on"}


async def aask_with_observability(action: Action, prompt: str) -> str:
    action_name = getattr(action, "name", action.__class__.__name__)

    cfg = ObservabilityConfig(
        langsmith_enabled=_env_flag("LANGSMITH_ENABLED"),
        langsmith_api_key=os.getenv("LANGSMITH_API_KEY") or os.getenv("LANGCHAIN_API_KEY"),
        langsmith_project=os.getenv("LANGSMITH_PROJECT") or "mgx-agent",
        langsmith_endpoint=os.getenv("LANGSMITH_ENDPOINT"),
    )
    langsmith_logger = get_langsmith_logger(cfg)

    started_at = datetime.now(timezone.utc)

    async with start_span(
        "mgx.aask",
        attributes={
            "mgx.action": action_name,
            "prompt.length": len(prompt),
        },
    ) as span:
        try:
            rsp = await action._aask(prompt)
            set_span_attributes(span, {"output.length": len(rsp) if rsp is not None else 0})
        except Exception as e:
            record_exception(span, e)
            if langsmith_logger is not None:
                await langsmith_logger.log_llm_call(
                    name=f"mgx.{action_name}",
                    provider="metagpt",
                    model="unknown",
                    prompt=prompt,
                    output="",
                    error=str(e),
                    start_time=started_at,
                    end_time=datetime.now(timezone.utc),
                    metadata={"action": action_name},
                )
            raise

    if langsmith_logger is not None:
        await langsmith_logger.log_llm_call(
            name=f"mgx.{action_name}",
            provider="metagpt",
            model="unknown",
            prompt=prompt,
            output=rsp or "",
            start_time=started_at,
            end_time=datetime.now(timezone.utc),
            metadata={"action": action_name},
        )

    return rsp


class AnalyzeTask(Action):
    """GÃ¶revi analiz et (stack-aware)"""
    
    PROMPT_TEMPLATE: str = """GÃ¶rev: {task}
{stack_context}

AÅŸaÄŸÄ±daki formatÄ± kullanarak gÃ¶revi analiz et:

KARMAÅIKLIK: [XS/S/M/L/XL]
Ã–NERÄ°LEN_STACK: [stack_id] - [kÄ±sa gerekÃ§e]
DOSYA_MANÄ°FESTO:
- [dosya1.ext]: [aÃ§Ä±klama]
- [dosya2.ext]: [aÃ§Ä±klama]
TEST_STRATEJÄ°SÄ°: [hangi test framework kullanÄ±lacak ve kaÃ§ test]

Kurallar:
- KARMAÅIKLIK: XS (tek fonksiyon), S (birkaÃ§ fonksiyon), M (modÃ¼l), L (Ã§oklu modÃ¼l), XL (sistem)
- Ã–NERÄ°LEN_STACK: {available_stacks} listesinden seÃ§
- DOSYA_MANÄ°FESTO: OluÅŸturulacak/deÄŸiÅŸtirilecek dosyalarÄ± listele
- TEST_STRATEJÄ°SÄ°: Hangi test framework ve kaÃ§ test yazÄ±lacaÄŸÄ±nÄ± belirt"""
    
    name: str = "AnalyzeTask"
    
    @llm_retry()
    async def run(self, task: str, target_stack: str = None) -> str:
        try:
            # Stack context oluÅŸtur
            from .stack_specs import STACK_SPECS, infer_stack_from_task
            
            available_stacks = ", ".join(STACK_SPECS.keys())
            
            if target_stack:
                stack_context = f"\nHedef Stack: {target_stack}"
            else:
                inferred = infer_stack_from_task(task)
                stack_context = f"\nÃ–nerilen Stack: {inferred} (gÃ¶rev aÃ§Ä±klamasÄ±ndan tahmin edildi)"
            
            prompt = self.PROMPT_TEMPLATE.format(
                task=task,
                stack_context=stack_context,
                available_stacks=available_stacks
            )
            rsp = await aask_with_observability(self, prompt)
            return rsp
        except Exception as e:
            logger.error(f"âŒ AnalyzeTask hatasÄ±: {e}")
            raise


class DraftPlan(Action):
    """Plan taslaÄŸÄ± oluÅŸtur (stack-aware)"""
    
    PROMPT_TEMPLATE: str = """GÃ¶rev: {task}

Analiz: {analysis}
{stack_info}

KÄ±sa ve Ã¶z plan yaz. SADECE ÅŸu formatÄ± kullan:

1. Kod yaz ({stack_name}) - Alex (Engineer)
2. Test yaz ({test_framework}) - Bob (Tester)  
3. Review yap - Charlie (Reviewer)

AÃ§Ä±klama veya detay YAZMA. SADECE numaralÄ± listeyi yaz."""
    
    name: str = "DraftPlan"
    
    @llm_retry()
    async def run(self, task: str, analysis: str, target_stack: str = None) -> str:
        try:
            # Stack bilgisi ekle
            from .stack_specs import get_stack_spec, infer_stack_from_task
            
            if not target_stack:
                target_stack = infer_stack_from_task(task)
            
            spec = get_stack_spec(target_stack)
            if spec:
                stack_info = f"\nStack: {spec.name}"
                stack_name = spec.language.upper()
                test_framework = spec.test_framework
            else:
                stack_info = ""
                stack_name = "Python"
                test_framework = "pytest"
            
            prompt = self.PROMPT_TEMPLATE.format(
                task=task,
                analysis=analysis,
                stack_info=stack_info,
                stack_name=stack_name,
                test_framework=test_framework
            )
            rsp = await aask_with_observability(self, prompt)
            return rsp
        except Exception as e:
            logger.error(f"âŒ DraftPlan hatasÄ±: {e}")
            raise


class WriteCode(Action):
    """Kod yaz (stack-aware, multi-language, FILE manifest)"""
    
    PROMPT_TEMPLATE: str = """
GÃ¶rev: {instruction}
Plan: {plan}
Stack: {stack_name}
Dil: {language}
{constraints_section}

{review_section}

{strict_mode_instructions}

ADIM 1 - DÃœÅÃœN (YALNIZCA METÄ°N):

- Bu gÃ¶revi nasÄ±l Ã§Ã¶zeceÄŸini 3â€“7 madde halinde kÄ±saca aÃ§Ä±kla.
- Hangi fonksiyonlarÄ±/component'leri yazacaÄŸÄ±nÄ± ve hangi kÃ¼tÃ¼phaneleri kullanacaÄŸÄ±nÄ± belirt.
- Edge case (uÃ§ durum) olarak neleri dikkate alacaÄŸÄ±nÄ± yaz.
- Hangi dosyalarÄ± oluÅŸturacaÄŸÄ±nÄ±/deÄŸiÅŸtireceÄŸini listele.
- Bu dÃ¼ÅŸÃ¼nce kÄ±smÄ±nda HÄ°Ã‡BÄ°R KOD yazma.

ADIM 2 - KODLA (FILE MANÄ°FEST FORMATINI KULLAN):

{file_format_instructions}

{revision_instructions}
"""
    
    FILE_FORMAT_STRICT: str = """
AÅŸaÄŸÄ±daki FILE manifest formatÄ±nÄ± KULLAN (aÃ§Ä±klama yasak, sadece dosyalar):

FILE: path/to/file1.{ext}
[dosya1 iÃ§eriÄŸi]

FILE: path/to/file2.{ext}
[dosya2 iÃ§eriÄŸi]

Ã–NEMLÄ°: 
- HER DOSYA "FILE: " ile baÅŸlamalÄ±
- Dosya yollarÄ± stack yapÄ±sÄ±na uygun olmalÄ±: {expected_structure}
- AÃ§Ä±klama veya yorum YAZMA, sadece FILE bloklarÄ±
"""
    
    FILE_FORMAT_NORMAL: str = """
AÅŸaÄŸÄ±daki FILE manifest formatÄ±nÄ± veya code block formatÄ±nÄ± kullan:

SEÃ‡ENEK 1 - FILE Manifest (Ã§oklu dosya iÃ§in):
FILE: path/to/file1.{ext}
[dosya1 iÃ§eriÄŸi]

FILE: path/to/file2.{ext}
[dosya2 iÃ§eriÄŸi]

SEÃ‡ENEK 2 - Code Block (tek dosya iÃ§in):
```{language}
# kod buraya
```

Ã–nerilen dosya yapÄ±sÄ±: {expected_structure}
"""
    
    name: str = "WriteCode"
    
    @llm_retry()
    async def run(
        self, 
        instruction: str, 
        plan: str = "", 
        review_notes: str = "",
        target_stack: str = None,
        constraints: list = None,
        strict_mode: bool = False,
        enable_validation: bool = True,
        max_validation_retries: int = 2
    ) -> str:
        try:
            # Stack bilgisi
            from .stack_specs import get_stack_spec, infer_stack_from_task
            from .guardrails import validate_output_constraints, build_revision_prompt
            
            if not target_stack:
                target_stack = infer_stack_from_task(instruction)
            
            spec = get_stack_spec(target_stack)
            if spec:
                stack_name = spec.name
                language = spec.language
                ext = spec.file_extensions[0] if spec.file_extensions else "txt"
                expected_structure = ", ".join(list(spec.project_layout.keys())[:5])
            else:
                stack_name = "Python"
                language = "python"
                ext = ".py"
                expected_structure = "src/, tests/"
            
            # Constraints
            constraints_section = ""
            if constraints:
                constraints_section = f"\nKÄ±sÄ±tlamalar:\n" + "\n".join(f"- {c}" for c in constraints)
            
            # Review notlarÄ±
            review_section = ""
            revision_instructions = ""
            if review_notes and review_notes.strip():
                review_section = f"""
Review NotlarÄ± (Ä°yileÅŸtirme Ã–nerileri):
{review_notes}
"""
                revision_instructions = f"""
Ã–NEMLÄ°: Bu bir dÃ¼zeltme turu. YukarÄ±daki review notlarÄ±nÄ± dikkate alarak mevcut kodu GÃœNCELLE / Ä°YÄ°LEÅTÄ°R.
Orijinal gÃ¶revi unutma: {instruction}
"""
            
            # Strict mode
            strict_mode_instructions = ""
            if strict_mode:
                file_format_instructions = self.FILE_FORMAT_STRICT.format(
                    ext=ext,
                    expected_structure=expected_structure
                )
                strict_mode_instructions = "âš ï¸ STRICT MODE: Sadece FILE bloklarÄ± yaz, hiÃ§bir aÃ§Ä±klama ekleme!"
            else:
                file_format_instructions = self.FILE_FORMAT_NORMAL.format(
                    ext=ext,
                    language=language,
                    expected_structure=expected_structure
                )
            
            # Main generation loop with validation retry
            validation_retry_count = 0
            final_output = None
            validation_result = None
            
            while validation_retry_count <= max_validation_retries:
                prompt = self.PROMPT_TEMPLATE.format(
                    instruction=instruction,
                    plan=plan,
                    stack_name=stack_name,
                    language=language,
                    constraints_section=constraints_section,
                    review_section=review_section,
                    strict_mode_instructions=strict_mode_instructions,
                    file_format_instructions=file_format_instructions,
                    revision_instructions=revision_instructions
                )
                rsp = await aask_with_observability(self, prompt)
                
                # Parse output based on mode
                if strict_mode:
                    output = rsp  # FILE manifest formatÄ±nda
                else:
                    output = self._parse_code(rsp, language)
                
                # Run validation if enabled
                if enable_validation:
                    validation_result = validate_output_constraints(
                        generated_output=output,
                        stack_spec=spec,
                        constraints=constraints,
                        strict_mode=strict_mode
                    )
                    
                    if validation_result.is_valid:
                        logger.info(f"âœ… Output validation passed: {validation_result.summary()}")
                        final_output = output
                        break
                    else:
                        # Validation failed
                        logger.warning(f"âŒ Output validation failed (attempt {validation_retry_count + 1}/{max_validation_retries + 1})")
                        logger.warning(f"Errors: {len(validation_result.errors)}, Warnings: {len(validation_result.warnings)}")
                        
                        for i, error in enumerate(validation_result.errors[:5], 1):
                            logger.warning(f"  {i}. {error}")
                        
                        if validation_retry_count < max_validation_retries:
                            # Build revision prompt with validation errors
                            revision_prompt = build_revision_prompt(validation_result, instruction)
                            review_notes = revision_prompt
                            validation_retry_count += 1
                            logger.info(f"ğŸ”„ Retrying with validation error feedback...")
                        else:
                            # Max retries reached
                            logger.error(f"âŒ Validation failed after {max_validation_retries + 1} attempts")
                            logger.error("Returning output with validation errors (marked as NEEDS_INFO)")
                            final_output = output
                            break
                else:
                    # Validation disabled
                    final_output = output
                    break
            
            # Log final validation status
            if enable_validation and validation_result and not validation_result.is_valid:
                error_summary = "\n".join(f"  - {e}" for e in validation_result.errors)
                logger.error(
                    f"âš ï¸ WriteCode returning output with validation errors:\n{error_summary}\n"
                    f"Task may require manual intervention (NEEDS_INFO)"
                )
            
            # Auto-format output if in FILE manifest mode
            if final_output and "FILE:" in final_output:
                final_output = self._format_output(final_output, target_stack, language)
            
            # Phase 11: Sandbox execution integration
            await self._execute_sandbox_testing(final_output, target_stack, language)
            
            return final_output
        except Exception as e:
            logger.error(f"âŒ WriteCode hatasÄ±: {e}")
            raise
    
    def _format_output(self, output: str, stack: str, language: str) -> str:
        """Format the output (placeholder for future implementation)."""
        return output

    async def _execute_sandbox_testing(
        self, 
        generated_code: str, 
        target_stack: str, 
        language: str
    ) -> bool:
        """
        Execute generated code in sandbox for testing and validation.
        
        This is part of Phase 11: Sandboxed Code Runner integration.
        Automatically runs tests after code generation to validate functionality.
        
        Args:
            generated_code: The generated code content
            target_stack: Technology stack identifier
            language: Programming language
            
        Returns:
            True if testing passed, False otherwise
        """
        try:
            # Only run sandbox testing in development/testing environments
            import os
            if os.getenv("DISABLE_SANDBOX_TESTING", "").lower() in ("true", "1", "yes"):
                logger.debug("ğŸ” Sandbox testing disabled via environment variable")
                return True
            
            logger.debug("ğŸ” Running sandbox testing for generated code")
            
            # Extract files from FILE manifest if present
            files = WriteCode._parse_file_manifest(generated_code)
            if not files:
                logger.debug("ğŸ” No files to test - single code block")
                return True
            
            # Map stack to sandbox language
            language_map = {
                'python': 'python',
                'nodejs': 'javascript',
                'javascript': 'javascript', 
                'typescript': 'javascript',
                'php': 'php',
                'react': 'javascript',
                'vue': 'javascript',
                'express': 'javascript',
            }
            
            sandbox_language = language_map.get(target_stack.lower(), language.lower())
            
            # Determine test command based on language and files
            test_command = self._determine_test_command(files, sandbox_language, target_stack)
            
            if not test_command:
                logger.debug("ğŸ” No test command determined for this stack/language")
                return True
            
            # Get the main code file content for execution
            main_code = self._extract_main_code(files, sandbox_language)
            if not main_code:
                logger.debug("ğŸ” No main code file found to test")
                return True
            
            # Execute in sandbox
            success = await self._run_sandbox_execution(
                code=main_code,
                command=test_command,
                language=sandbox_language,
                timeout=60  # 60 second timeout for testing
            )
            
            if success:
                logger.info("âœ… Sandbox testing passed")
            else:
                logger.warning("âš ï¸ Sandbox testing failed - check logs for details")
                return False
            
            # Phase 12: Run Quality Gates after successful sandbox execution
            quality_gate_success = await self._run_quality_gates_after_sandbox(
                files=files,
                workspace_id="test-workspace",  # TODO: Get from context
                project_id="test-project",  # TODO: Get from context
                task_run_id=None  # TODO: Get from context
            )
            
            if not quality_gate_success:
                logger.warning("âš ï¸ Quality gates failed - code needs revision")
                return False
            
            logger.info("âœ… Quality gates passed")
            return True
            
        except Exception as e:
            logger.warning(f"âš ï¸ Sandbox testing error (non-blocking): {e}")
            return True  # Don't fail the main task if testing fails
    
    def _determine_test_command(
        self, 
        files: List[Tuple[str, str]], 
        language: str, 
        stack: str
    ) -> Optional[str]:
        """
        Determine appropriate test command for the generated code.
        
        Args:
            files: List of (filepath, content) tuples
            language: Programming language
            stack: Technology stack
            
        Returns:
            Test command string or None
        """
        # Check for existing test files in FILE manifest
        test_files = [f[0] for f in files if any(test_indicator in f[0].lower() 
                       for test_indicator in ['test_', '_test.', 'spec.', '.test.', '.spec.'])]
        
        if test_files:
            # Test files exist, use them
            if language == 'python':
                # Check for pytest
                if any('pytest' in f.lower() or 'test_' in f.lower() for f in test_files):
                    return "python -m pytest"
                return "python -m unittest discover"
            elif language == 'javascript':
                # Check for package.json
                package_json_files = [f for f in files if 'package.json' in f[0]]
                if package_json_files:
                    return "npm test"
                return "node test.js"
            elif language == 'php':
                return "vendor/bin/phpunit"
        
        # No test files found - run basic syntax validation
        if language == 'python':
            # Try to compile the code for syntax validation
            try:
                import ast
                for file_path, content in files:
                    if file_path.endswith('.py'):
                        ast.parse(content)
                return None  # Syntax is valid, no additional testing needed
            except SyntaxError:
                return "python -m py_compile"
        
        elif language == 'javascript':
            # Basic Node.js syntax check
            return "node --check"
        
        elif language == 'php':
            # PHP syntax check
            return "php -l"
        
        return None
    
    def _extract_main_code(self, files: List[Tuple[str, str]], language: str) -> Optional[str]:
        """
        Extract main code file content for sandbox execution.
        
        Args:
            files: List of (filepath, content) tuples
            language: Programming language
            
        Returns:
            Main code content or None
        """
        # Language-specific main file patterns
        main_patterns = {
            'python': ['main.py', 'app.py', 'index.py', 'server.py', '__main__.py'],
            'javascript': ['main.js', 'app.js', 'index.js', 'server.js', 'app.js'],
            'php': ['index.php', 'main.php', 'app.php', 'server.php'],
        }
        
        # Look for main files
        main_files = []
        for file_path, content in files:
            file_name = file_path.split('/')[-1]
            if file_name in main_patterns.get(language, []):
                main_files.append((file_path, content))
        
        # If no main files found, take the first non-test file
        if not main_files:
            for file_path, content in files:
                if not any(test_indicator in file_path.lower() 
                          for test_indicator in ['test_', '_test.', 'spec.', '.test.', '.spec.']):
                    main_files.append((file_path, content))
                    break
        
        # Return the content of the first main file
        if main_files:
            return main_files[0][1]
        
        return None
    
    async def _run_sandbox_execution(
        self, 
        code: str, 
        command: str, 
        language: str, 
        timeout: int = 30
    ) -> bool:
        """
        Execute code in sandbox environment.
        
        Args:
            code: Source code to execute
            command: Command to run
            language: Programming language
            timeout: Execution timeout in seconds
            
        Returns:
            True if execution was successful
        """
        try:
            # Import sandbox runner only when needed
            try:
                from backend.services.sandbox import get_sandbox_runner
                runner = get_sandbox_runner()
            except ImportError:
                logger.debug("ğŸ” Sandbox runner not available - skipping execution")
                return True
            
            # Create execution ID
            import uuid
            execution_id = str(uuid.uuid4())
            
            # Execute in sandbox
            result = await runner.execute_code(
                execution_id=execution_id,
                code=code,
                command=command,
                language=language,
                timeout=timeout,
                memory_limit_mb=512,
                workspace_id="test-workspace",  # TODO: Get from context
                project_id="test-project",  # TODO: Get from context
            )
            
            # Log results
            if result.get('success'):
                logger.info(f"âœ… Sandbox execution successful: {command}")
                if result.get('stdout'):
                    logger.debug(f"Output: {result['stdout'][:200]}")
                return True
            else:
                logger.warning(f"âš ï¸ Sandbox execution failed: {command}")
                if result.get('stderr'):
                    logger.warning(f"Error: {result['stderr'][:200]}")
                return False
                
        except Exception as e:
            logger.warning(f"âš ï¸ Sandbox execution error: {e}")
            return False
    
    async def _run_quality_gates_after_sandbox(
        self,
        files: List[Tuple[str, str]],
        workspace_id: str,
        project_id: str,
        task_run_id: Optional[str] = None
    ) -> bool:
        """
        Run quality gates after successful sandbox execution.
        
        Args:
            files: List of (filepath, content) tuples
            workspace_id: Workspace ID
            project_id: Project ID  
            task_run_id: Task run ID
            
        Returns:
            True if all quality gates pass
        """
        try:
            # Import quality gate manager
            try:
                from backend.services.quality_gates import get_gate_manager
                gate_manager = await get_gate_manager()
            except ImportError:
                logger.debug("ğŸ” Quality gate manager not available - skipping")
                return True
            
            # Determine which gates to run based on file types
            gate_types = self._determine_quality_gates_for_files(files)
            
            if not gate_types:
                logger.debug("ğŸ” No applicable quality gates found for these files")
                return True
            
            logger.info(f"ğŸ—ï¸ Running quality gates: {gate_types}")
            
            # Run quality gates
            result = await gate_manager.evaluate_gates(
                workspace_id=workspace_id,
                project_id=project_id,
                gate_types=gate_types,
                task_run_id=task_run_id,
                working_directory="/tmp/test",  # Use temp directory for testing
            )
            
            if not result.get("success", False):
                logger.warning(f"âš ï¸ Quality gate evaluation failed: {result.get('error', 'Unknown error')}")
                return False
            
            # Check if evaluation passed
            if result.get("passed", False):
                blocking_failures = result.get("blocking_failures", [])
                if blocking_failures:
                    logger.warning(f"âš ï¸ Quality gates failed with blocking failures: {blocking_failures}")
                    return False
                else:
                    logger.info("âœ… All quality gates passed")
                    return True
            else:
                blocking_failures = result.get("blocking_failures", [])
                logger.warning(f"âš ï¸ Quality gates failed: {blocking_failures}")
                
                # Log detailed results for debugging
                for gate_type, gate_result in result.get("results", {}).items():
                    if not gate_result.get("passed", True):
                        logger.warning(f"  - {gate_type}: {gate_result.get('status', 'unknown')} - {gate_result.get('error_message', 'No details')}")
                
                return False
                
        except Exception as e:
            logger.warning(f"âš ï¸ Quality gate execution error: {e}")
            return False  # Fail closed - if quality gates can't run, assume failure
    
    def _determine_quality_gates_for_files(self, files: List[Tuple[str, str]]) -> List[str]:
        """
        Determine which quality gates to run based on file types.
        
        Args:
            files: List of (filepath, content) tuples
            
        Returns:
            List of gate types to run
        """
        gate_types = []
        file_extensions = set()
        
        # Collect file extensions
        for filepath, _ in files:
            if filepath:
                ext = filepath.split('.')[-1].lower() if '.' in filepath else ''
                if ext:
                    file_extensions.add(ext)
        
        # Determine gates based on file types
        if any(ext in file_extensions for ext in ['js', 'jsx', 'ts', 'tsx']):
            gate_types.extend(["lint", "security"])  # JavaScript/TypeScript files
        
        if any(ext in file_extensions for ext in ['py']):
            gate_types.extend(["lint", "coverage", "security", "complexity", "type_check"])  # Python files
        
        if any(ext in file_extensions for ext in ['php']):
            gate_types.extend(["lint", "coverage", "security", "complexity"])  # PHP files
        
        # Always include performance gate if there are any executable files
        if file_extensions:
            gate_types.append("performance")
        
        # Always include contract gate for any web/API projects
        if any(ext in file_extensions for ext in ['js', 'jsx', 'ts', 'tsx', 'py', 'php', 'go', 'java']):
            gate_types.append("contract")
        
        # Remove duplicates while preserving order
        return list(dict.fromkeys(gate_types))
    
    @staticmethod
    def _parse_file_manifest(manifest: str) -> List[Tuple[str, str]]:
        """
        Parse FILE manifest format into list of (filepath, content) tuples.
        
        Format:
        FILE: path/to/file1.ext
        [content line 1]
        [content line 2]
        
        FILE: path/to/file2.ext
        [content]
        
        Returns:
            List of (file_path, content) tuples
        """
        files = []
        current_file = None
        current_content = []
        
        for line in manifest.split('\n'):
            if line.startswith('FILE: '):
                # Save previous file if exists
                if current_file and current_content:
                    content = '\n'.join(current_content)
                    files.append((current_file, content))
                
                # Start new file
                current_file = line[6:].strip()  # Remove 'FILE: ' prefix
                current_content = []
            elif current_file:
                # Add line to current file
                current_content.append(line)
        
        # Save last file
        if current_file and current_content:
            content = '\n'.join(current_content).rstrip()
            files.append((current_file, content))
        
        return files
    
    @staticmethod
    def _parse_code(rsp: str, language: str = "python") -> str:
        """Code block'tan kodu Ã§Ä±kar (backward compatibility)"""
        # Ã–nce FILE manifest formatÄ±nÄ± kontrol et
        if "FILE:" in rsp:
            return rsp  # FILE manifest formatÄ±nda, olduÄŸu gibi dÃ¶ndÃ¼r
        
        # Dile Ã¶zel pattern'ler
        patterns = [
            rf"```{language}(.*?)```",
            r"```(.*)```",
        ]
        
        for pattern in patterns:
            match = re.search(pattern, rsp, re.DOTALL)
            if match:
                return match.group(1).strip()
        
        # HiÃ§bir pattern match etmezse, olduÄŸu gibe dÃ¶ndÃ¼r
        return rsp


class WriteTest(Action):
    """Test yaz (stack-aware)"""
    
    PROMPT_TEMPLATE: str = """
    Kod:
    {code}
    
    Stack: {stack_name}
    Test Framework: {test_framework}
    
    Ã–NEMLÄ°: Bu kod iÃ§in {test_framework} kullanarak TAM OLARAK {k} ADET unit test yaz.
    DAHA FAZLA YAZMA! Sadece {k} adet test yaz.
    
    Kurallar:
    1. TAM OLARAK {k} adet test yaz (daha fazla deÄŸil!)
    2. Her test farklÄ± bir senaryoyu test etmeli:
       - Pozitif senaryo (normal kullanÄ±m)
       - Negatif senaryo (hata durumlarÄ±)
       - Edge case (sÄ±nÄ±r deÄŸerleri)
    3. AynÄ± testi tekrar yazma - her test benzersiz olmalÄ±
    4. Test isimleri aÃ§Ä±klayÄ±cÄ± olsun
    5. {test_framework} syntax'Ä±nÄ± kullan
    
    Sadece {k} adet test yaz, daha fazla deÄŸil!
    
    ```{language}
    {test_template}
    ```
    
    UYARI: Sadece {k} adet test yaz, daha fazla yazma!
    """
    
    name: str = "WriteTest"
    
    @staticmethod
    def _parse_code(rsp: str, language: str = "python") -> str:
        """Code block'tan test kodunu Ã§Ä±kar"""
        patterns = [
            rf"```{language}(.*?)```",
            r"```(.*)```",
        ]
        
        for pattern in patterns:
            match = re.search(pattern, rsp, re.DOTALL)
            if match:
                return match.group(1).strip()
        
        return rsp.strip()
    
    @staticmethod
    def _limit_tests(code: str, k: int) -> str:
        """
        Test kodundan sadece ilk k adet test fonksiyonunu al.
        LLM daha fazla test yazsa bile sadece k adet test dÃ¶ndÃ¼rÃ¼r.
        
        Args:
            code: Test kodu
            k: Maksimum test sayÄ±sÄ±
            
        Returns:
            Sadece k adet test iÃ§eren kod
        """
        lines = code.splitlines()
        result = []
        test_count = 0
        in_test_function = False
        
        for i, line in enumerate(lines):
            # Test fonksiyonu baÅŸlangÄ±cÄ±nÄ± tespit et
            if re.match(r'^\s*def\s+test_', line):
                if test_count >= k:
                    # K adet test bulundu, daha fazlasÄ±nÄ± alma
                    break
                test_count += 1
                in_test_function = True
                result.append(line)
            elif in_test_function:
                # Test fonksiyonu iÃ§indeyiz
                result.append(line)
                # Bir sonraki test fonksiyonu veya dosya sonu gelirse dur
                if i + 1 < len(lines):
                    next_line = lines[i + 1]
                    if re.match(r'^\s*def\s+test_', next_line):
                        # Bir sonraki test baÅŸlÄ±yor, eÄŸer k adet test bulunduysa dur
                        if test_count >= k:
                            break
            else:
                # Test fonksiyonu dÄ±ÅŸÄ±ndayÄ±z (import, class tanÄ±mlarÄ± vs.)
                result.append(line)
        
        # EÄŸer hiÃ§ test bulunamadÄ±ysa orijinal kodu dÃ¶ndÃ¼r
        if test_count == 0:
            return code
        
        return "\n".join(result)
    
    def _get_test_template(self, test_framework: str, language: str, k: int) -> str:
        """Test framework'e gÃ¶re template dÃ¶ndÃ¼r"""
        templates = {
            "pytest": """import pytest

# Test 1: [aÃ§Ä±klama]
def test_1():
    # kod

# Test 2: [aÃ§Ä±klama]
def test_2():
    # kod

# Test {k}: [aÃ§Ä±klama]
def test_{k}():
    # kod""",
            
            "jest": """import {{ describe, it, expect }} from '@jest/globals';

describe('TestSuite', () => {{
  it('Test 1: [aÃ§Ä±klama]', () => {{
    // kod
  }});
  
  it('Test 2: [aÃ§Ä±klama]', () => {{
    // kod
  }});
  
  it('Test {k}: [aÃ§Ä±klama]', () => {{
    // kod
  }});
}});""",
            
            "vitest": """import {{ describe, it, expect }} from 'vitest';

describe('TestSuite', () => {{
  it('Test 1: [aÃ§Ä±klama]', () => {{
    // kod
  }});
  
  it('Test 2: [aÃ§Ä±klama]', () => {{
    // kod
  }});
  
  it('Test {k}: [aÃ§Ä±klama]', () => {{
    // kod
  }});
}});""",
            
            "phpunit": """<?php

use PHPUnit\\Framework\\TestCase;

class MyTest extends TestCase
{{
    public function test1(): void
    {{
        // kod
    }}
    
    public function test2(): void
    {{
        // kod
    }}
    
    public function test{k}(): void
    {{
        // kod
    }}
}}""",
        }
        
        template = templates.get(test_framework.lower(), templates["pytest"])
        return template.replace("{k}", str(k))
    
    @llm_retry()
    async def run(self, code: str, k: int = 3, target_stack: str = None) -> str:
        try:
            # Stack bilgisi
            from .stack_specs import get_stack_spec, infer_stack_from_task
            
            if target_stack:
                spec = get_stack_spec(target_stack)
                if spec:
                    test_framework = spec.test_framework
                    language = spec.language
                    stack_name = spec.name
                else:
                    test_framework = "pytest"
                    language = "python"
                    stack_name = "Python"
            else:
                test_framework = "pytest"
                language = "python"
                stack_name = "Python"
            
            test_template = self._get_test_template(test_framework, language, k)
            
            prompt = self.PROMPT_TEMPLATE.format(
                code=code,
                k=k,
                stack_name=stack_name,
                test_framework=test_framework,
                language=language,
                test_template=test_template
            )
            rsp = await aask_with_observability(self, prompt)
            raw_code = self._parse_code(rsp, language)
            # Post-process: Test sayÄ±sÄ±nÄ± k ile sÄ±nÄ±rla (LLM daha fazla yazsa bile)
            limited_code = self._limit_tests(raw_code, k)
            logger.debug(f"ğŸ“Š WriteTest: {k} adet test sÄ±nÄ±rÄ± uygulandÄ± ({test_framework})")
            return limited_code
        except Exception as e:
            logger.error(f"âŒ WriteTest hatasÄ±: {e}")
            raise


class ReviewCode(Action):
    """Kodu incele ve geri bildirim ver (stack-aware)"""
    
    PROMPT_TEMPLATE: str = """
    Kod:
    {code}
    
    Testler:
    {tests}
    
    Stack: {stack_name}
    {stack_specific_checks}
    
    Bu kodu ve testleri DÄ°KKATLÄ°CE incele:
    1. Kod kalitesi nasÄ±l? Hata yÃ¶netimi var mÄ±? Input validation var mÄ±?
    2. Test coverage yeterli mi? Edge case'ler test edilmiÅŸ mi?
    3. Docstring'ler/Comment'ler var mÄ±? Kod dokÃ¼mantasyonu yeterli mi?
    4. Stack-specific best practices uygulanmÄ±ÅŸ mÄ±?
    5. GÃ¼venlik: Environment variables, secrets, input sanitization kontrol edilmiÅŸ mi?
    6. Build/Test/Run komutlarÄ± doÄŸru mu? (package.json, composer.json, requirements.txt vs.)
    7. Ä°yileÅŸtirme gereken noktalar var mÄ±?
    
    Ã–NEMLÄ°: EÄŸer kodda eksiklikler, hatalar veya iyileÅŸtirme gereken noktalar varsa MUTLAKA "DEÄÄ°ÅÄ°KLÄ°K GEREKLÄ°" yaz.
    Sadece kod mÃ¼kemmel ve hiÃ§bir sorun yoksa "ONAYLANDI" yaz.
    
    SONUÃ‡: [ONAYLANDI / DEÄÄ°ÅÄ°KLÄ°K GEREKLÄ°]
    
    YORUMLAR:
    - [yorum 1]
    - [yorum 2]
    - [yorum 3]
    """
    
    name: str = "ReviewCode"
    
    def _get_stack_checks(self, stack_id: str) -> str:
        """Stack-specific kontrol listesi"""
        checks = {
            "express-ts": """
Kontrol listesi (Express-TS):
- Middleware sÄ±rasÄ± doÄŸru mu? (body-parser, cors, helmet)
- Error handling middleware var mÄ±?
- TypeScript tipleri tam mÄ±?
- .env iÃ§in dotenv kullanÄ±lmÄ±ÅŸ mÄ±?""",
            
            "nestjs": """
Kontrol listesi (NestJS):
- Module/Controller/Service yapÄ±sÄ± doÄŸru mu?
- Dependency Injection kullanÄ±lmÄ±ÅŸ mÄ±?
- DTO validation var mÄ±?
- Exception filters uygun mu?""",
            
            "laravel": """
Kontrol listesi (Laravel):
- Eloquent relationships doÄŸru mu?
- Request validation kullanÄ±lmÄ±ÅŸ mÄ±?
- Route tanÄ±mlarÄ± RESTful mi?
- Migration dosyalarÄ± var mÄ±?""",
            
            "fastapi": """
Kontrol listesi (FastAPI):
- Pydantic model'ler kullanÄ±lmÄ±ÅŸ mÄ±?
- Async/await doÄŸru kullanÄ±lmÄ±ÅŸ mÄ±?
- Dependency Injection var mÄ±?
- Response model'ler tanÄ±mlanmÄ±ÅŸ mÄ±?""",
            
            "react-vite": """
Kontrol listesi (React-Vite):
- Component yapÄ±sÄ± temiz mi?
- Props type checking (TypeScript) var mÄ±?
- State management doÄŸru mu?
- useEffect dependency array'leri doÄŸru mu?""",
            
            "nextjs": """
Kontrol listesi (Next.js):
- App Router / Pages Router kullanÄ±mÄ± doÄŸru mu?
- Server/Client component ayrÄ±mÄ± yapÄ±lmÄ±ÅŸ mÄ±?
- API routes doÄŸru tanÄ±mlanmÄ±ÅŸ mÄ±?
- Metadata/SEO ayarlarÄ± var mÄ±?""",
            
            "vue-vite": """
Kontrol listesi (Vue-Vite):
- Composition API doÄŸru kullanÄ±lmÄ±ÅŸ mÄ±?
- Reactive state management uygun mu?
- Component props/emits tanÄ±mlanmÄ±ÅŸ mÄ±?
- Script setup syntax kullanÄ±lmÄ±ÅŸ mÄ±?""",
        }
        
        return checks.get(stack_id, "")
    
    @llm_retry()
    async def run(self, code: str, tests: str, target_stack: str = None) -> str:
        try:
            # Stack bilgisi
            from .stack_specs import get_stack_spec
            
            if target_stack:
                spec = get_stack_spec(target_stack)
                if spec:
                    stack_name = spec.name
                    stack_specific_checks = self._get_stack_checks(target_stack)
                else:
                    stack_name = "Python"
                    stack_specific_checks = ""
            else:
                stack_name = "Python"
                stack_specific_checks = ""
            
            prompt = self.PROMPT_TEMPLATE.format(
                code=code,
                tests=tests,
                stack_name=stack_name,
                stack_specific_checks=stack_specific_checks
            )
            rsp = await self._aask(prompt)
            return rsp
        except Exception as e:
            logger.error(f"âŒ ReviewCode hatasÄ±: {e}")
            raise
