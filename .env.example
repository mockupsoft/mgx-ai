# =============================================================================
# MGX Agent - Docker Compose Environment Variables
# =============================================================================
# Copy this file to .env and customize for your deployment
#
# SECURITY WARNING:
# - Change all JWT_SECRET and API_KEY values for production
# - Change MinIO credentials (S3_ACCESS_KEY_ID, S3_SECRET_ACCESS_KEY)
# - Change database passwords (DB_PASSWORD)
# - Use strong, randomly-generated values (see below for generation)
#
# Generate secure secrets:
#   JWT_SECRET: openssl rand -hex 32
#   API_KEY: openssl rand -hex 32
#   DB_PASSWORD: openssl rand -hex 16
#   S3_SECRET_ACCESS_KEY: openssl rand -base64 32
#
# =============================================================================

# =============================================================================
# Core Application Settings
# =============================================================================

# Deployment environment: development, staging, production
MGX_ENV=production

# API server host and port (host should be 0.0.0.0 for Docker)
MGX_PORT=8000
MGX_BASE_URL=http://localhost:8000

# Number of uvicorn workers (1-4 for small deployments, 4-8 for larger)
MGX_WORKERS=4

# Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL
MGX_LOG_LEVEL=INFO

# =============================================================================
# Security - CHANGE THESE FOR PRODUCTION
# =============================================================================

# JWT secret for authentication (generate with: openssl rand -hex 32)
# CHANGE THIS: openssl rand -hex 32
JWT_SECRET=change-me-in-production-use-openssl-rand-hex-32

# API key for service-to-service authentication
# CHANGE THIS: openssl rand -hex 32
API_KEY=change-me-in-production-use-openssl-rand-hex-32

# =============================================================================
# MGX Agent Configuration
# =============================================================================

# Maximum number of rounds for task execution (1-20)
MGX_MAX_ROUNDS=5

# Maximum number of revision rounds for output validation (0-5)
MGX_MAX_REVISION_ROUNDS=2

# Maximum memory size for agent context (10-500 MB)
MGX_MAX_MEMORY_SIZE=50

# Enable response caching (true/false)
MGX_ENABLE_CACHING=true

# Cache backend: none, memory, or redis
# - memory: In-process LRU cache (single worker)
# - redis: Distributed cache (multi-worker, requires Redis)
# - none: No caching
MGX_CACHE_BACKEND=redis

# Maximum number of cache entries for memory backend
MGX_CACHE_MAX_ENTRIES=10000

# Cache entry time-to-live in seconds (60-86400)
MGX_CACHE_TTL_SECONDS=3600

# =============================================================================
# PostgreSQL Database Configuration
# =============================================================================

# Database host (use 'postgres' for Docker Compose internal networking)
DB_HOST=postgres

# Database port
DB_PORT=5432

# Database name
DB_NAME=mgx

# Database user (create this user in PostgreSQL)
DB_USER=mgx

# Database password (CHANGE THIS FOR PRODUCTION)
# CHANGE THIS: openssl rand -hex 16
DB_PASSWORD=mgx

# Connection pool settings
DB_POOL_SIZE=10
DB_MAX_OVERFLOW=20

# Async connection URL (auto-generated from above settings)
# Format: postgresql+asyncpg://user:password@host:port/database
# Leave empty to auto-generate from DB_* variables above
# DATABASE_URL=postgresql+asyncpg://mgx:mgx@postgres:5432/mgx

# =============================================================================
# Redis Cache Configuration
# =============================================================================

# Redis host (use 'redis' for Docker Compose internal networking)
REDIS_HOST=redis

# Redis port
REDIS_PORT=6379

# Redis cache TTL in seconds (should match MGX_CACHE_TTL_SECONDS)
REDIS_CACHE_TTL=3600

# Full Redis URL (auto-generated if empty)
# Leave empty to auto-generate from REDIS_HOST and REDIS_PORT
# REDIS_URL=redis://redis:6379/0

# =============================================================================
# S3/MinIO Object Storage Configuration
# =============================================================================

# S3 endpoint URL (use 'http://minio:9000' for Docker Compose internal)
# For external S3 (AWS), use: https://s3.amazonaws.com or s3.region.amazonaws.com
S3_ENDPOINT_URL=http://minio:9000

# AWS region (any valid AWS region, default: us-east-1)
S3_REGION=us-east-1

# S3 bucket name for artifacts
S3_BUCKET=mgx-artifacts

# S3 access key (MinIO root user or AWS access key)
# CHANGE THIS FOR PRODUCTION
S3_ACCESS_KEY_ID=minioadmin

# S3 secret key (MinIO root password or AWS secret key)
# CHANGE THIS FOR PRODUCTION: openssl rand -base64 32
S3_SECRET_ACCESS_KEY=minioadmin

# Use TLS for S3 connection (true for AWS, false for local MinIO)
S3_SECURE=false

# =============================================================================
# Application Defaults
# =============================================================================

# Default workspace name created on startup
DEFAULT_WORKSPACE_NAME=default

# Default project name created in the default workspace
DEFAULT_PROJECT_NAME=default

# =============================================================================
# Optional Integrations
# =============================================================================

# Enable Kafka event streaming (requires --profile kafka)
KAFKA_ENABLED=false

# Kafka broker addresses (if KAFKA_ENABLED=true)
# For Docker Compose: kafka:29092 (internal), localhost:9092 (external)
# KAFKA_BROKERS=kafka:29092

# Kafka topic for event streaming
# KAFKA_TOPIC_EVENTS=mgx-events

# Enable OpenTelemetry observability
OTEL_ENABLED=false

# OpenTelemetry exporter endpoint (if OTEL_ENABLED=true)
# OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317

# Sentry error tracking (optional)
# SENTRY_DSN=https://key@sentry.io/project-id

# =============================================================================
# GitHub Integration (Optional)
# =============================================================================

# GitHub App authentication (recommended for production)
# Follow: https://docs.github.com/en/apps/creating-github-apps/creating-github-apps/creating-a-github-app

# GitHub App ID
# GITHUB_APP_ID=123456

# GitHub App Client ID
# GITHUB_CLIENT_ID=Iv1.abcdef123456

# Path to GitHub App private key PEM file (relative to /app)
# GITHUB_PRIVATE_KEY_PATH=/run/secrets/github_app_private_key.pem

# Fallback: GitHub Personal Access Token (if App auth not configured)
# Required scopes: repo, workflow, gist, read:user, write:repo_hook
# GITHUB_PAT=ghp_...

# Local directory for cached git clones (must be writable)
GITHUB_CLONE_CACHE_DIR=/tmp/mgx-agent-repos

# =============================================================================
# Multi-Agent System Configuration
# =============================================================================

# Enable the multi-agent system (required for agent features)
AGENTS_ENABLED=false

# Comma-separated list of Python modules to auto-load agent definitions
# Example: AGENT_REGISTRY_MODULES=myapp.agents.builders,myapp.agents.reviewers
# Each module should register agent classes on import
AGENT_REGISTRY_MODULES=

# Maximum number of concurrent agent instances per workspace
AGENT_MAX_CONCURRENCY=10

# Maximum number of context versions to retain per agent context
# Older versions beyond this limit may be pruned
AGENT_CONTEXT_HISTORY_LIMIT=100

# =============================================================================
# Debug & Monitoring
# =============================================================================

# Enable debug mode (affects logging verbosity)
DEBUG=false

# Application log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=INFO

# =============================================================================
# Advanced Database Settings (Usually not needed)
# =============================================================================

# PostgreSQL shared_buffers (for tuning database performance)
# Default 256MB, increase for larger deployments
# PG_SHARED_BUFFERS=256MB

# PostgreSQL effective_cache_size (for query planner tuning)
# Default 1GB, should be about 1/4 of available RAM
# PG_EFFECTIVE_CACHE_SIZE=1GB

# PostgreSQL work_mem (memory per sort/hash operation)
# Default 16MB, increase if you have memory and run complex queries
# PG_WORK_MEM=16MB

# PostgreSQL max connections
# Default 200, adjust based on connection pool size
# PG_MAX_CONNECTIONS=200

# =============================================================================
# Advanced Redis Settings (Usually not needed)
# =============================================================================

# Redis persistence: appendonly yes/no
# Default: yes (AOF persistence enabled)
# REDIS_APPENDONLY=yes

# Redis appendfsync: always, everysec, no
# Default: everysec (balance between durability and performance)
# REDIS_APPENDFSYNC=everysec

# =============================================================================
# Notes for Production Deployment
# =============================================================================

# 1. SECURITY:
#    - Run: openssl rand -hex 32 | xargs -I {} sed -i 's/change-me-in-production/{}/g' .env
#    - Rotate secrets regularly
#    - Use Docker secrets or encrypted vaults for sensitive data
#    - Enable TLS for all external connections

# 2. PERFORMANCE:
#    - Monitor resources: docker compose stats
#    - Increase MGX_WORKERS based on CPU cores (CPU count - 1)
#    - Increase DB_POOL_SIZE for high concurrency (20-50)
#    - Enable Redis for distributed caching

# 3. BACKUPS:
#    - Regular PostgreSQL backups: docker exec mgx-postgres pg_dump -U mgx mgx > backup.sql
#    - MinIO backups: sync volumes to external storage
#    - Test restore procedures regularly

# 4. MONITORING:
#    - Use: docker compose logs -f mgx-ai
#    - Monitor health: docker compose ps
#    - Set up alerts on unhealthy services
#    - Enable OpenTelemetry for detailed observability

# 5. SCALING:
#    - Use docker compose --profile kafka for event streaming
#    - Enable Redis for distributed caching
#    - Consider S3-compatible external storage for MinIO
#    - Use managed PostgreSQL for databases
#    - Implement reverse proxy (nginx/caddy) in front of port 8000

# =============================================================================
