# TEM Agent Ä°yileÅŸtirme Rehberi

Bu dokÃ¼man, TEM Agent projesinin kod review raporunda belirlenen sorunlarÄ± Ã§Ã¶zmek iÃ§in adÄ±m adÄ±m rehber ve Ã¶rnekler saÄŸlar.

---

## ðŸ“‘ Ä°Ã§indekiler

1. [Modularization (BÃ¶lÃ¼mleme)](#1-modularization)
2. [Test AltyapÄ±sÄ±](#2-test-altyapÄ±sÄ±)
3. [Code Refactoring](#3-code-refactoring)
4. [DokÃ¼mantasyon](#4-dokÃ¼mantasyon)
5. [Performance Optimization](#5-performance-optimization)
6. [GÃ¼venlik IyileÅŸtirmeleri](#6-gÃ¼venlik-iyileÅŸtirmeleri)

---

## 1. Modularization

### Mevcut Durum
```
examples/
â””â”€â”€ mgx_style_team.py (2392 satÄ±r - HER ÅžEY BÄ°RDE!)
```

### Hedef Durum
```
mgx_agent/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ constants.py           # Magic numbers
â”œâ”€â”€ config.py             # TeamConfig, TaskComplexity
â”œâ”€â”€ metrics.py            # TaskMetrics
â”œâ”€â”€ actions.py            # Action sÄ±nÄ±flarÄ±
â”œâ”€â”€ roles.py              # Role sÄ±nÄ±flarÄ±
â”œâ”€â”€ adapter.py            # MetaGPT adaptasyonu
â”œâ”€â”€ team.py               # MGXStyleTeam orchestrator
â”œâ”€â”€ utils.py              # Helper fonksiyonlar
â””â”€â”€ cli.py                # CLI entry points
```

### Implementation Plan

#### Step 1: constants.py
```python
# mgx_agent/constants.py
"""Proje sabitleri"""

# Task Complexity Levels
COMPLEXITY_XS = "XS"
COMPLEXITY_S = "S"
COMPLEXITY_M = "M"
COMPLEXITY_L = "L"
COMPLEXITY_XL = "XL"

# Default Values
DEFAULT_MAX_ROUNDS = 5
DEFAULT_MAX_REVISION_ROUNDS = 2
DEFAULT_MAX_MEMORY_SIZE = 50
DEFAULT_CACHE_TTL_SECONDS = 3600
DEFAULT_INVESTMENT = 3.0
DEFAULT_BUDGET_MULTIPLIER = 1.0

# Performance Settings
PROGRESS_BAR_LENGTH = 20
RELEVANT_MEMORY_LIMIT = 5
DEFAULT_TEST_COUNT = 3
REVIEW_NOTES_MAX_LENGTH = 500

# Retry Settings
RETRY_MAX_ATTEMPTS = 3
RETRY_MIN_WAIT = 2
RETRY_MAX_WAIT = 10

# Model Pricing (Ã¶rnek - gerÃ§ek fiyatlar eklenecek)
MODEL_PRICING = {
    "gpt-4": {
        "prompt": 0.03,
        "completion": 0.06
    },
    "gpt-3.5-turbo": {
        "prompt": 0.0005,
        "completion": 0.0015
    }
}

# Magic Strings
JSON_START_MARKER = "---JSON_START---"
JSON_END_MARKER = "---JSON_END---"
COMPLEXITY_PATTERN = r"KARMAÅžIKLIK:\s*(XS|S|M|L|XL)"
CODE_BLOCK_PATTERN = r"```(?:python)?\s*(.*?)\s*```"
```

#### Step 2: Modular File Structure

Mevcut dosyayÄ± ÅŸu ÅŸekilde ayÄ±r:

**config.py:**
```python
# mgx_agent/config.py
from enum import Enum
from pydantic import BaseModel, Field, field_validator, ConfigDict
import yaml
import logging

logger = logging.getLogger(__name__)

class LogLevel(str, Enum):
    """Log seviyeleri"""
    DEBUG = "debug"
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"

class TaskComplexity:
    """GÃ¶rev karmaÅŸÄ±klÄ±k seviyeleri"""
    XS = "XS"
    S = "S"
    M = "M"
    L = "L"
    XL = "XL"

class TeamConfig(BaseModel):
    """MGX Style Team konfigÃ¼rasyonu"""
    # ... (mevcut implementation)
    
    @field_validator('max_rounds')
    @classmethod
    def validate_max_rounds(cls, v):
        if v < 1:
            raise ValueError("max_rounds en az 1 olmalÄ±")
        return v
```

**metrics.py:**
```python
# mgx_agent/metrics.py
from dataclasses import dataclass, field
from datetime import datetime

@dataclass
class TaskMetrics:
    """GÃ¶rev metrikleri"""
    task_name: str
    start_time: float
    end_time: float = 0.0
    success: bool = False
    complexity: str = "XS"
    token_usage: int = 0
    estimated_cost: float = 0.0
    revision_rounds: int = 0
    error_message: str = ""
    
    @property
    def duration_seconds(self) -> float:
        """GÃ¶rev sÃ¼resi (saniye)"""
        return self.end_time - self.start_time if self.end_time else 0.0
    
    @property
    def duration_formatted(self) -> str:
        """FormatlanmÄ±ÅŸ sÃ¼re"""
        secs = self.duration_seconds
        if secs < 60:
            return f"{secs:.1f}s"
        elif secs < 3600:
            return f"{secs/60:.1f}m"
        else:
            return f"{secs/3600:.1f}h"
    
    def to_dict(self) -> dict:
        """MetriÄŸi dict olarak dÃ¶ndÃ¼r"""
        return {
            "task_name": self.task_name,
            "duration": self.duration_formatted,
            "success": self.success,
            "complexity": self.complexity,
            "token_usage": self.token_usage,
            "estimated_cost": f"${self.estimated_cost:.4f}",
            "revision_rounds": self.revision_rounds,
            "error": self.error_message if self.error_message else None
        }
```

**adapter.py:**
```python
# mgx_agent/adapter.py
"""MetaGPT adaptasyonu ve soyutlama"""

class MetaGPTAdapter:
    """MetaGPT'nin internal API'sini soyutlayan adapter"""
    
    @staticmethod
    def get_memory_store(role):
        """Role'dan memory store'u gÃ¼venli ÅŸekilde al"""
        if not hasattr(role, "rc"):
            return None
        if not hasattr(role.rc, "memory"):
            return None
        return role.rc.memory
    
    # ... (diÄŸer metodlar)
```

**utils.py:**
```python
# mgx_agent/utils.py
"""YardÄ±mcÄ± fonksiyonlar"""

import re
from typing import Optional

def parse_code_blocks(text: str) -> list:
    """Metinden Python kod bloklarÄ±nÄ± Ã§Ä±kar"""
    if not text:
        return []
    
    pattern = r"```(?:python)?\s*(.*?)\s*```"
    matches = re.findall(pattern, text, re.DOTALL)
    return [m.strip() for m in matches if m.strip()]

def parse_json_block(text: str, start_marker: str = "---JSON_START---", 
                     end_marker: str = "---JSON_END---") -> Optional[dict]:
    """GÃ¶mÃ¼lÃ¼ JSON'u parse et"""
    if start_marker not in text or end_marker not in text:
        return None
    
    try:
        json_str = text.split(start_marker)[1].split(end_marker)[0].strip()
        return json.loads(json_str)
    except (json.JSONDecodeError, IndexError, ValueError) as e:
        logger.warning(f"JSON parse hatasÄ±: {e}")
        return None

def extract_complexity(text: str) -> str:
    """Metinden karmaÅŸÄ±klÄ±k seviyesini Ã§Ä±kar"""
    from constants import COMPLEXITY_PATTERN, COMPLEXITY_XS
    
    m = re.search(COMPLEXITY_PATTERN, text.upper())
    return m.group(1) if m else COMPLEXITY_XS

def print_phase_header(phase: str, emoji: str = "ðŸ”„"):
    """Faz baÅŸlÄ±ÄŸÄ± yazdÄ±r"""
    print(f"\n{'='*60}")
    print(f"{emoji} {phase}")
    print(f"{'='*60}")

def print_step_progress(step: int, total: int, description: str, role=None):
    """AdÄ±m adÄ±m progress gÃ¶ster
    
    Args:
        step: Mevcut adÄ±m numarasÄ±
        total: Toplam adÄ±m sayÄ±sÄ±
        description: Ä°ÅŸlem aÃ§Ä±klamasÄ±
        role: Role instance (team referansÄ± iÃ§in)
    """
    if role and hasattr(role, '_team_ref') and hasattr(role._team_ref, '_print_progress'):
        role._team_ref._print_progress(step, total, description)
        return
    
    bar_length = 20
    filled = int(bar_length * step / total)
    bar = "â–ˆ" * filled + "â–‘" * (bar_length - filled)
    percent = int(100 * step / total)
    print(f"\r[{bar}] {percent}% - {description}", end="", flush=True)
    if step == total:
        print()
```

### Verification Checklist

- [ ] `constants.py` oluÅŸturuldu ve tÃ¼m magic numbers taÅŸÄ±ndÄ±
- [ ] `config.py` ayrÄ± dosya olarak Ã§alÄ±ÅŸÄ±yor
- [ ] `metrics.py` ayrÄ± dosya olarak Ã§alÄ±ÅŸÄ±yor
- [ ] `adapter.py` ayrÄ± dosya olarak Ã§alÄ±ÅŸÄ±yor
- [ ] `utils.py` helper fonksiyonlarÄ± iÃ§eriyor
- [ ] `__init__.py` imports'larÄ± expose ediyor
- [ ] TÃ¼m imports dÃ¼zeltildi ve relative imports kullanÄ±lÄ±yor
- [ ] Tests hepsini import edebildiÄŸini doÄŸruluyor

---

## 2. Test AltyapÄ±sÄ±

### Test Framework Kurulumu

```bash
# pyproject.toml
[tool.pytest.ini_options]
testpaths = ["tests"]
asyncio_mode = "auto"
markers = [
    "unit: unit tests",
    "integration: integration tests",
    "slow: slow tests",
]

[tool.coverage.run]
source = ["mgx_agent"]
omit = ["*/tests/*"]

[tool.coverage.report]
exclude_lines = [
    "pragma: no cover",
    "def __repr__",
    "raise AssertionError",
    "raise NotImplementedError",
    "if __name__ == .__main__.:",
]
```

### Test Strukturu

```
tests/
â”œâ”€â”€ conftest.py                      # Shared fixtures
â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ test_config.py              # Config validation
â”‚   â”œâ”€â”€ test_metrics.py             # Metrics calculation
â”‚   â”œâ”€â”€ test_utils.py               # Utility functions
â”‚   â”œâ”€â”€ test_adapter.py             # MetaGPT adapter
â”‚   â”œâ”€â”€ test_actions.py             # LLM action classes
â”‚   â””â”€â”€ test_roles.py               # Role classes
â”œâ”€â”€ integration/
â”‚   â”œâ”€â”€ test_team_workflow.py       # Full workflow
â”‚   â”œâ”€â”€ test_revision_loop.py       # Revision mechanism
â”‚   â””â”€â”€ test_incremental.py         # Incremental features
â”œâ”€â”€ fixtures/
â”‚   â”œâ”€â”€ mock_responses.py           # Mock LLM responses
â”‚   â”œâ”€â”€ sample_code.py              # Test code samples
â”‚   â””â”€â”€ sample_projects/            # Test projects
â””â”€â”€ README.md
```

### Ã–rnek Test Cases

**tests/conftest.py:**
```python
import pytest
import asyncio
from unittest.mock import AsyncMock, MagicMock

@pytest.fixture
def event_loop():
    """Async test loop"""
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()

@pytest.fixture
def mock_llm():
    """Mock LLM for testing"""
    mock = AsyncMock()
    mock._aask = AsyncMock(return_value="Mock response")
    return mock

@pytest.fixture
def sample_code():
    """Sample Python code for testing"""
    return '''
def multiply_list(numbers):
    """Listedeki sayÄ±larÄ±n Ã§arpÄ±mÄ±nÄ± hesapla"""
    result = 1
    for n in numbers:
        result *= n
    return result
'''

@pytest.fixture
def sample_tests():
    """Sample test code"""
    return '''
def test_multiply_list_positive():
    assert multiply_list([2, 3, 4]) == 24

def test_multiply_list_single():
    assert multiply_list([5]) == 5

def test_multiply_list_with_zero():
    assert multiply_list([2, 0, 3]) == 0
'''
```

**tests/unit/test_config.py:**
```python
import pytest
from mgx_agent.config import TeamConfig, TaskComplexity, LogLevel

class TestTaskComplexity:
    """TaskComplexity enum'i test et"""
    
    def test_complexity_values(self):
        """KarmaÅŸÄ±klÄ±k seviyeleri tanÄ±mlanmÄ±ÅŸ mÄ±?"""
        assert TaskComplexity.XS == "XS"
        assert TaskComplexity.S == "S"
        assert TaskComplexity.M == "M"
        assert TaskComplexity.L == "L"
        assert TaskComplexity.XL == "XL"

class TestTeamConfig:
    """TeamConfig validation test'leri"""
    
    def test_default_config(self):
        """Default config deÄŸerleri doÄŸru mu?"""
        config = TeamConfig()
        assert config.max_rounds == 5
        assert config.enable_caching is True
        assert config.human_reviewer is False
    
    def test_invalid_max_rounds(self):
        """0 rounds reject edilmeli"""
        with pytest.raises(ValueError):
            TeamConfig(max_rounds=0)
    
    def test_invalid_investment(self):
        """$0.5'den az investment reject edilmeli"""
        with pytest.raises(ValueError):
            TeamConfig(default_investment=0.2)
    
    def test_high_budget_multiplier_warning(self, caplog):
        """YÃ¼ksek budget multiplier'Ä± warning verir"""
        config = TeamConfig(budget_multiplier=15.0)
        assert config.budget_multiplier == 15.0
        # Warning kontrolÃ¼ (eÄŸer uygulandÄ±ysa)
    
    @pytest.mark.parametrize("level", [
        LogLevel.DEBUG, LogLevel.INFO, LogLevel.WARNING, LogLevel.ERROR
    ])
    def test_log_levels(self, level):
        """TÃ¼m log seviyeleri kabul edilmeli"""
        config = TeamConfig(log_level=level)
        assert config.log_level == level
    
    def test_config_to_dict(self):
        """Config dict'e dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmeli"""
        config = TeamConfig(max_rounds=10)
        d = config.to_dict()
        assert d["max_rounds"] == 10
        assert "enable_caching" in d
    
    def test_config_from_dict(self):
        """Dict'ten config oluÅŸturulmalÄ±"""
        d = {"max_rounds": 8, "human_reviewer": True}
        config = TeamConfig.from_dict(d)
        assert config.max_rounds == 8
        assert config.human_reviewer is True
    
    @pytest.mark.asyncio
    async def test_config_yaml_roundtrip(self, tmp_path):
        """YAML save/load cycle Ã§alÄ±ÅŸmalÄ±"""
        config = TeamConfig(
            max_rounds=12,
            budget_multiplier=1.5,
            human_reviewer=True
        )
        
        path = tmp_path / "config.yaml"
        config.save_yaml(str(path))
        
        loaded = TeamConfig.from_yaml(str(path))
        assert loaded.max_rounds == 12
        assert loaded.budget_multiplier == 1.5
        assert loaded.human_reviewer is True
```

**tests/unit/test_utils.py:**
```python
import pytest
from mgx_agent.utils import (
    parse_code_blocks,
    parse_json_block,
    extract_complexity,
)

class TestParseCodeBlocks:
    """Code block parsing test'leri"""
    
    def test_single_code_block(self):
        """Tek bir kod bloÄŸu parse edilmeli"""
        text = """
        ```python
        def hello():
            return "world"
        ```
        """
        blocks = parse_code_blocks(text)
        assert len(blocks) == 1
        assert "def hello" in blocks[0]
    
    def test_multiple_code_blocks(self):
        """Birden fazla kod bloÄŸu parse edilmeli"""
        text = """
        ```python
        x = 1
        ```
        Some text
        ```python
        y = 2
        ```
        """
        blocks = parse_code_blocks(text)
        assert len(blocks) == 2
    
    def test_code_block_without_language(self):
        """Dil belirtilmeden kod bloÄŸu da Ã§alÄ±ÅŸmalÄ±"""
        text = "```\nx = 1\n```"
        blocks = parse_code_blocks(text)
        assert len(blocks) == 1
        assert "x = 1" in blocks[0]
    
    def test_empty_string(self):
        """BoÅŸ string boÅŸ liste dÃ¶ndÃ¼rmeli"""
        assert parse_code_blocks("") == []
        assert parse_code_blocks(None) == []

class TestParseJsonBlock:
    """JSON block parsing test'leri"""
    
    def test_valid_json(self):
        """GeÃ§erli JSON parse edilmeli"""
        text = """
        ---JSON_START---
        {"key": "value", "number": 42}
        ---JSON_END---
        """
        result = parse_json_block(text)
        assert result == {"key": "value", "number": 42}
    
    def test_invalid_json(self):
        """Invalid JSON None dÃ¶ndÃ¼rmeli"""
        text = "---JSON_START---{invalid json}---JSON_END---"
        result = parse_json_block(text)
        assert result is None
    
    def test_missing_markers(self):
        """Marker yoksa None dÃ¶ndÃ¼rmeli"""
        result = parse_json_block('{"key": "value"}')
        assert result is None

class TestExtractComplexity:
    """Complexity extraction test'leri"""
    
    @pytest.mark.parametrize("text,expected", [
        ("KARMAÅžIKLIK: XS", "XS"),
        ("karmaÅŸÄ±klÄ±k: M", "M"),
        ("GÃ¶rev karmaÅŸÄ±klÄ±ÄŸÄ±: L", "L"),
        ("", "XS"),  # Default
        ("HiÃ§ karmaÅŸÄ±klÄ±k yok", "XS"),  # Default
    ])
    def test_extract_complexity(self, text, expected):
        """FarklÄ± formatlardaki karmaÅŸÄ±klÄ±k Ã§Ä±karÄ±lmalÄ±"""
        assert extract_complexity(text) == expected
```

---

## 3. Code Refactoring

### Execute() Fonksiyonunu BÃ¶l

**Problem:**
```python
async def execute(self, n_round: int = None, max_revision_rounds: int = None) -> str:
    # 226 satÄ±r - TOO LONG!
    # Derin nesting, karmaÅŸÄ±k logic
```

**Ã‡Ã¶zÃ¼m:**
```python
# mgx_agent/team.py
async def execute(self, n_round: int = None, max_revision_rounds: int = None) -> str:
    """GÃ¶revi Ã§alÄ±ÅŸtÄ±r - orchestration katmanÄ±"""
    if not self._validate_execution_prerequisites():
        return "âŒ Ã–n koÅŸullar saÄŸlanmadÄ±"
    
    # Initialize
    budget, metric = await self._initialize_execution(n_round)
    
    try:
        # Phase 1: First execution
        await self._run_first_execution_round(budget)
        
        # Phase 2: Revision loops
        revision_count = await self._run_revision_loops(
            max_revision_rounds=max_revision_rounds,
            budget=budget
        )
        
        # Phase 3: Finalize
        return await self._finalize_execution(metric, revision_count)
        
    except Exception as e:
        return await self._handle_execution_error(e, metric)
    
    finally:
        metric.end_time = time.time()
        if self.metrics is not None:
            self.metrics.append(metric)
            self._show_metrics_report(metric)

async def _validate_execution_prerequisites(self) -> bool:
    """Ã‡alÄ±ÅŸtÄ±rma Ã¶n koÅŸullarÄ±nÄ± kontrol et"""
    if not self.plan_approved and not self.config.auto_approve_plan:
        logger.warning("Plan henÃ¼z onaylanmadÄ±")
        return False
    return True

async def _initialize_execution(self, n_round: int) -> tuple:
    """Ã‡alÄ±ÅŸtÄ±rmayÄ± initialize et"""
    start_time = time.time()
    metric = TaskMetrics(
        task_name=self.current_task[:50] if self.current_task else "Unknown",
        start_time=start_time
    )
    
    complexity = self._get_complexity_from_plan()
    budget = self._tune_budget(complexity)
    metric.complexity = complexity
    
    if n_round is None:
        n_round = budget["n_round"]
    
    return budget, metric

async def _run_first_execution_round(self, budget: dict):
    """Ä°lk execution turunu Ã§alÄ±ÅŸtÄ±r"""
    print_phase_header("GÃ¶rev YÃ¼rÃ¼tme", "ðŸš€")
    print(f"ðŸ“Š KarmaÅŸÄ±klÄ±k: {budget.get('investment')} "
          f"Investment: ${budget['investment']}")
    
    # Complete planning phase
    for role in self.team.env.roles.values():
        if hasattr(role, 'complete_planning'):
            role.complete_planning()
    
    # Run team
    self.team.invest(investment=budget["investment"])
    await self.team.run(n_round=budget.get("n_round", 3))
    
    # Cleanup
    self.cleanup_memory()

async def _run_revision_loops(self, max_revision_rounds: int, budget: dict) -> int:
    """Revision dÃ¶ngÃ¼lerini Ã§alÄ±ÅŸtÄ±r"""
    revision_count = 0
    last_review_hash = None
    
    while revision_count < max_revision_rounds:
        code, tests, review = self._collect_raw_results()
        
        if not self._should_continue_revision(review, last_review_hash):
            break
        
        if "DEÄžÄ°ÅžÄ°KLÄ°K GEREKLÄ°" in review.upper():
            revision_count += 1
            await self._run_revision_improvements(code, review, budget)
            last_review_hash = hashlib.md5(review.encode()).hexdigest()
        else:
            print("\nâœ… Review ONAYLANDI - DÃ¼zeltme gerekmiyor.")
            break
    
    return revision_count

def _should_continue_revision(self, review: str, last_review_hash: str) -> bool:
    """Revision dÃ¶ngÃ¼sÃ¼ne devam edilmeli mi?"""
    if not review or not review.strip():
        logger.warning("Review bulunamadÄ±")
        return False
    
    review_hash = hashlib.md5(review.encode()).hexdigest()
    if review_hash == last_review_hash:
        logger.warning("AynÄ± review tekrar geldi - sonsuz dÃ¶ngÃ¼ detected")
        return False
    
    return True
```

### Conditional Nesting'i Azalt

**Before:**
```python
if not instruction:
    for m in all_messages:
        content = m.content if hasattr(m, 'content') else str(m)
        if "---JSON_START---" in content and "---JSON_END---" in content:
            try:
                json_str = content.split("---JSON_START---")[1].split("---JSON_END---")[0].strip()
                data = json.loads(json_str)
                if "task" in data and "plan" in data:
                    instruction = data["task"]
                    plan = data["plan"]
                    break
            except (json.JSONDecodeError, IndexError, ValueError):
                pass
```

**After:**
```python
def _extract_task_spec_from_messages(self, messages) -> Optional[dict]:
    """Mesajlardan task spec Ã§Ä±kar - null-safe"""
    for msg in messages:
        spec = self._extract_json_spec(msg.content)
        if spec:
            return spec
    return None

def _extract_json_spec(self, content: str) -> Optional[dict]:
    """Content'den JSON spec'i parse et"""
    if not content:
        return None
    
    spec = parse_json_block(content)
    if spec and "task" in spec and "plan" in spec:
        return spec
    
    return None

# Usage:
spec = self._extract_task_spec_from_messages(all_messages)
if spec:
    instruction = spec["task"]
    plan = spec["plan"]
```

---

## 4. DokÃ¼mantasyon

### README.md Åžablonu

```markdown
# MGX Style Multi-Agent Team (TEM Agent)

MetaGPT aÃ§Ä±k kaynak kodunun Ã¼zerine geliÅŸtirilen, dÃ¶rt rol iÃ§eren bir multi-agent sistem.

## ðŸš€ Features

- **4 Specialized Roles:** Mike (Planner), Alex (Engineer), Bob (Tester), Charlie (Reviewer)
- **Automatic Task Complexity Assessment:** XS/S/M/L/XL levels
- **Intelligent Revision Loops:** AI-driven code improvements
- **Metrics & Cost Tracking:** Monitor token usage and estimated costs
- **Flexible Configuration:** Pydantic-based config with validation
- **Human-in-the-Loop:** Optional human review integration
- **Incremental Development:** Add features or fix bugs in existing projects

## ðŸ“¦ Installation

```bash
pip install -e .
```

### Requirements

- Python 3.8+
- MetaGPT
- Pydantic v2
- Tenacity

## ðŸŽ¯ Quick Start

### Basic Usage

```python
import asyncio
from mgx_agent.team import MGXStyleTeam

async def main():
    team = MGXStyleTeam()
    task = "Bir Python fonksiyonu yaz: Listedeki sayÄ±larÄ±n Ã§arpÄ±mÄ±nÄ± hesapla"
    
    # Analiz ve plan oluÅŸtur
    await team.analyze_and_plan(task)
    
    # PlanÄ± onayla
    team.approve_plan()
    
    # GÃ¶revi Ã§alÄ±ÅŸtÄ±r
    await team.execute()

asyncio.run(main())
```

### CLI Usage

```bash
# Normal mode
python -m mgx_agent

# Human reviewer mode
python -m mgx_agent --human

# Custom task
python -m mgx_agent --task "Your custom task here"

# Add feature
python -m mgx_agent --add-feature "Add authentication" --project-path ./my_project

# Fix bug
python -m mgx_agent --fix-bug "TypeError: x is undefined" --project-path ./my_project
```

## âš™ï¸ Configuration

```python
from mgx_agent.config import TeamConfig

config = TeamConfig(
    max_rounds=5,                    # Max execution rounds
    max_revision_rounds=2,           # Max revision iterations
    enable_caching=True,             # Cache task analysis
    human_reviewer=False,            # Human in the loop
    default_investment=3.0,          # Budget in dollars
    budget_multiplier=1.0,           # Adjust budget
)

team = MGXStyleTeam(config=config)
```

### Config from YAML

```python
config = TeamConfig.from_yaml("config.yaml")
team = MGXStyleTeam(config=config)
```

## ðŸ“Š Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         CLI / Main Entry            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      MGXStyleTeam Orchestrator      â”‚
â”‚   (Task spec, memory, metrics)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚        â”‚        â”‚
    â–¼        â–¼        â–¼
  Mike    Alex    Bob    Charlie
(Planner) (Eng) (Test) (Review)
    â”‚        â”‚        â”‚        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   MetaGPTAdapterâ”‚
    â”‚  (Safe API Access)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸ§ª Testing

```bash
# Run all tests
pytest

# With coverage
pytest --cov=mgx_agent

# Specific test file
pytest tests/unit/test_config.py

# Async tests
pytest tests/integration/test_team_workflow.py
```

## ðŸ“ˆ Metrics

The team tracks:
- Task completion time
- Token usage
- Estimated costs
- Revision rounds
- Success/failure status

```python
metrics = team.get_metrics_summary()
print(metrics)
```

## ðŸ”„ Revision Loop

The system automatically improves code:

1. Alex writes code
2. Bob writes tests
3. Charlie reviews
4. If issues found â†’ Alex revises (max 2-5 rounds)
5. Repeat until approved

## ðŸ›¡ï¸ Error Handling

- Automatic LLM retry (3 attempts with exponential backoff)
- Graceful fallbacks for missing data
- Comprehensive error logging

## ðŸ“ Logging

```python
import logging
from mgx_agent.config import LogLevel, TeamConfig

config = TeamConfig(log_level=LogLevel.DEBUG, verbose=True)
team = MGXStyleTeam(config=config)
```

## ðŸ› Known Limitations

- Test coverage: Work in progress
- Multi-LLM mode: Requires manual config (see issue #123)
- Human reviewer: Currently terminal-based

## ðŸ“š API Reference

### MGXStyleTeam

#### `analyze_and_plan(task: str) -> str`
Analyzes task and creates a plan

#### `approve_plan() -> bool`
Approves current plan for execution

#### `execute(n_round: int = None) -> str`
Executes the approved task

#### `add_feature(feature: str, project_path: str) -> str`
Adds a feature to existing project

#### `fix_bug(bug_description: str, project_path: str) -> str`
Fixes a bug in existing project

## ðŸ¤ Contributing

1. Write tests first (TDD)
2. Follow PEP 8
3. Add docstrings
4. Update CHANGELOG

## ðŸ“„ License

MIT
```

### ARCHITECTURE.md

```markdown
# TEM Agent Architecture

## Overview

TEM (Turkish Engineer Multi-Agent) is a MetaGPT-based system that coordinates 4 specialized AI agents to develop software.

## Component Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          CLI/Interface Layer            â”‚
â”‚  (main.py, incremental_main.py)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Orchestration Layer                   â”‚
â”‚     (MGXStyleTeam)                        â”‚
â”‚  - Task spec management                   â”‚
â”‚  - Memory cleanup                         â”‚
â”‚  - Metrics tracking                       â”‚
â”‚  - Revision loop control                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                   â”‚               â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
â”‚  Roles     â”‚  â”‚  Actions  â”‚  â”‚  Config   â”‚
â”‚  Layer     â”‚  â”‚  Layer    â”‚  â”‚  Layer    â”‚
â”‚            â”‚  â”‚           â”‚  â”‚           â”‚
â”‚ - Mike     â”‚  â”‚ - Analyze â”‚  â”‚ - Teams   â”‚
â”‚   (Planner)â”‚  â”‚ - DraftPlnâ”‚  â”‚ - Tasks   â”‚
â”‚ - Alex     â”‚  â”‚ - Write   â”‚  â”‚ - Budget  â”‚
â”‚   (Engine) â”‚  â”‚ - WriteTstâ”‚  â”‚ - Metrics â”‚
â”‚ - Bob      â”‚  â”‚ - Review  â”‚  â”‚           â”‚
â”‚   (Tester) â”‚  â”‚           â”‚  â”‚           â”‚
â”‚ - Charlie  â”‚  â”‚           â”‚  â”‚           â”‚
â”‚   (Review) â”‚  â”‚           â”‚  â”‚           â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚                  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  MetaGPT Adapter        â”‚
                    â”‚  (Safe API Access)      â”‚
                    â”‚                         â”‚
                    â”‚ - get_memory_store()    â”‚
                    â”‚ - get_messages()        â”‚
                    â”‚ - add_message()         â”‚
                    â”‚ - clear_memory()        â”‚
                    â”‚ - get_news()            â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   MetaGPT Framework     â”‚
                    â”‚                         â”‚
                    â”‚ - Context               â”‚
                    â”‚ - Team                  â”‚
                    â”‚ - Role                  â”‚
                    â”‚ - Action                â”‚
                    â”‚ - Message               â”‚
                    â”‚ - Memory                â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Data Flow

### Phase 1: Analysis & Planning

```
User Input (Task)
    â”‚
    â–¼
Mike.analyze_and_plan()
    â”‚
    â”œâ”€> AnalyzeTask (LLM)
    â”‚       â””â”€> Complexity Level (XS/S/M/L/XL)
    â”‚
    â””â”€> DraftPlan (LLM)
            â””â”€> Plan Steps
                â”‚
                â–¼
        Save to task_spec (single source of truth)
                â”‚
                â–¼
        Return to User
```

### Phase 2: Execution

```
Approved Task Spec
    â”‚
    â”œâ”€> Alex.run()
    â”‚   â””â”€> WriteCode (LLM)
    â”‚       â””â”€> Code Output
    â”‚
    â”œâ”€> Bob.run()
    â”‚   â””â”€> WriteTest (LLM with Code)
    â”‚       â””â”€> Test Output
    â”‚
    â””â”€> Charlie.run()
        â””â”€> ReviewCode (LLM with Code + Tests)
            â””â”€> Review Output
                â”‚
                â”œâ”€> "ONAYLANDI" â†’ Done
                â””â”€> "DEÄžÄ°ÅžÄ°KLÄ°K GEREKLÄ°"
                    â”‚
                    â–¼
            Enter Revision Loop
```

### Phase 3: Revision (if needed)

```
Review with Issues
    â”‚
    â–¼
MGXStyleTeam.set_task_spec() with review_notes
    â”‚
    â–¼
Alex.run() with review_notes (revision prompt)
    â”‚
    â”œâ”€> Updated Code
    â”œâ”€> Bob.run() â†’ Updated Tests
    â””â”€> Charlie.run() â†’ New Review
        â”‚
        â””â”€> Check again (max 2-5 rounds)
```

## Key Abstractions

### MetaGPTAdapter

Why needed:
- MetaGPT internals are not stable
- Private attributes (_memory) are implementation details
- API might change between versions

Pattern:
```python
# Direct access (WRONG - fragile):
messages = role.rc.memory._messages  # â† Private!

# Adapter access (CORRECT - safe):
memory_store = MetaGPTAdapter.get_memory_store(role)
messages = MetaGPTAdapter.get_messages(memory_store)
```

### Task Spec (Single Source of Truth)

Instead of parsing messages repeatedly, we maintain one `current_task_spec`:

```python
current_task_spec = {
    "task": "Original task description",
    "plan": "Step-by-step plan",
    "complexity": "M",
    "is_revision": False,
    "review_notes": ""  # Only set during revision
}
```

Benefits:
- Consistent state
- No message parsing errors
- Efficient lookups

### Memory Management

- **Token Efficiency:** Only keep relevant memories
- **Cleanup Strategy:** Keep last N messages per role
- **Cache:** Task analysis results with TTL

## Configuration Flow

```
Default Config
    â”‚
    â”œâ”€> User provides TeamConfig()
    â”‚   â”‚
    â”‚   â””â”€> Pydantic Validation
    â”‚       (ge=1, le=20, etc.)
    â”‚
    â”œâ”€> Load YAML (optional)
    â”‚   â”‚
    â”‚   â””â”€> Override defaults
    â”‚
    â””â”€> Multi-LLM Config (optional)
        â”‚
        â””â”€> Load model-specific configs
```

## Metrics & Monitoring

```
Task Execution
    â”‚
    â–¼
TaskMetrics Object
    â”‚
    â”œâ”€> start_time
    â”œâ”€> end_time
    â”œâ”€> success/failure
    â”œâ”€> complexity
    â”œâ”€> token_usage (estimated)
    â”œâ”€> estimated_cost
    â”œâ”€> revision_rounds
    â””â”€> error_message
        â”‚
        â–¼
    Display Report
    Save to metrics list
    Export to JSON/CSV
```

## Error Handling Strategy

```
LLM Call
    â”‚
    â”œâ”€> Success â†’ Return result
    â”‚
    â””â”€> Failure
        â”‚
        â”œâ”€> Retry 1 (wait 2-5s)
        â”œâ”€> Retry 2 (wait 5-10s)
        â”œâ”€> Retry 3 (wait 10s)
        â”‚
        â””â”€> All failed â†’ Return error / use fallback
```

## Extension Points

### Adding a New Role

```python
class Dave(Role):
    """Security Reviewer"""
    name: str = "Dave"
    profile: str = "SecurityReviewer"
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.set_actions([SecurityReview])
        self._watch([WriteCode, WriteTest])
    
    async def _act(self) -> Message:
        # Implementation
        pass
```

### Adding a New Action

```python
class SecurityAudit(Action):
    """Security audit action"""
    name: str = "SecurityAudit"
    
    @llm_retry()
    async def run(self, code: str) -> str:
        prompt = "Review this code for security issues..."
        return await self._aask(prompt)
```

## Known Issues & Limitations

1. **Multi-LLM Mode:** Config loading works but role distribution may not
2. **Streaming:** Flag exists but not implemented
3. **Token Counting:** Estimated, not actual
4. **Human Input:** Basic terminal input, no validation
5. **Path Handling:** Assumes write permission to current dir

See CODE_REVIEW_REPORT.md for detailed analysis.
```

---

## 5. Performance Optimization

### Memory Access Optimization

```python
# mgx_agent/team.py

class MGXStyleTeam:
    def __init__(self, ...):
        # Cache role references
        self._roles_cache = {}
        self._regenerate_role_cache()
    
    def _regenerate_role_cache(self):
        """Role referanslarÄ±nÄ± cache'le"""
        if hasattr(self.team, 'env') and hasattr(self.team.env, 'roles'):
            self._roles_cache = dict(self.team.env.roles)
    
    def _collect_results_optimized(self) -> tuple:
        """Optimized result collection"""
        code_content = ""
        test_content = ""
        review_content = ""
        
        # Use cached roles
        for role in self._roles_cache.values():
            mem_store = MetaGPTAdapter.get_memory_store(role)
            if mem_store is None:
                continue
            
            # Get only last message (iteration-free)
            messages = MetaGPTAdapter.get_messages(mem_store)
            if not messages:
                continue
            
            last_msg = messages[-1]  # O(1) instead of O(n)
            
            if last_msg.role == "Engineer":
                code_content = last_msg.content
            elif last_msg.role == "Tester":
                test_content = last_msg.content
            elif last_msg.role == "Reviewer":
                review_content = last_msg.content
        
        return code_content, test_content, review_content
```

### Async Optimization

```python
# mgx_agent/utils.py

async def load_configs_parallel(config_paths: dict) -> dict:
    """KonfigÃ¼rasyon dosyalarÄ±nÄ± paralel yÃ¼kle"""
    import asyncio
    
    async def load_one(name: str, path: str):
        loop = asyncio.get_event_loop()
        return name, await loop.run_in_executor(None, Config.from_home, path)
    
    tasks = [load_one(name, path) for name, path in config_paths.items()]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    return {name: config for name, config in results if not isinstance(config, Exception)}
```

---

## 6. GÃ¼venlik Ä°yileÅŸtirmeleri

### Input Validation

```python
# mgx_agent/utils.py

import re
from pathlib import Path

def sanitize_path(user_input: str, base_dir: str = "output") -> str:
    """KullanÄ±cÄ± input'undan gÃ¼venli path oluÅŸtur"""
    # Sadece alphanumeric + underscore + hyphen
    sanitized = re.sub(r'[^a-zA-Z0-9_-]', '', user_input)
    
    if not sanitized:
        sanitized = "output"
    
    path = Path(base_dir) / sanitized
    
    # Path traversal kontrolÃ¼
    try:
        path.resolve().relative_to(Path(base_dir).resolve())
    except ValueError:
        raise ValueError(f"Invalid path: {user_input}")
    
    return str(path)

def validate_task_description(task: str, max_length: int = 10000) -> str:
    """GÃ¶rev aÃ§Ä±klamasÄ±nÄ± validate et"""
    if not task or not isinstance(task, str):
        raise ValueError("Task must be a non-empty string")
    
    if len(task) > max_length:
        raise ValueError(f"Task exceeds max length of {max_length}")
    
    # Injection kontrolleri (basit)
    dangerous_patterns = [
        r"exec\(",
        r"eval\(",
        r"__import__",
        r"system\(",
        r"popen\(",
    ]
    
    task_lower = task.lower()
    for pattern in dangerous_patterns:
        if re.search(pattern, task_lower):
            raise ValueError(f"Suspicious pattern detected: {pattern}")
    
    return task
```

### Safe File Operations

```python
# mgx_agent/utils.py

import shutil
from pathlib import Path

def safe_write_file(path: str, content: str, max_size: int = 10 * 1024 * 1024) -> bool:
    """DosyayÄ± gÃ¼venli ÅŸekilde yaz"""
    try:
        # Size check
        if len(content) > max_size:
            logger.warning(f"Content size {len(content)} exceeds max {max_size}")
            return False
        
        # Path validation
        path = Path(path)
        path.parent.mkdir(parents=True, exist_ok=True)
        
        # Backup existing file
        if path.exists():
            backup = path.with_suffix(path.suffix + f".bak_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
            shutil.copy2(path, backup)
        
        # Atomic write
        with open(path, "w", encoding="utf-8") as f:
            f.write(content)
        
        logger.info(f"File written safely: {path}")
        return True
        
    except Exception as e:
        logger.error(f"Safe file write failed: {e}")
        return False
```

---

## ðŸ“‹ Implementation Checklist

### Phase 1 (1-2 weeks)
- [ ] Modularize codebase into mgx_agent package
- [ ] Create constants.py with all magic numbers
- [ ] Set up pytest framework with conftest
- [ ] Write 20 unit tests for config
- [ ] Write 10 unit tests for utils
- [ ] Write README.md
- [ ] Complete human-in-the-loop feature
- [ ] Add .gitignore

### Phase 2 (1-2 weeks)
- [ ] Write 30+ integration tests
- [ ] Refactor execute() method
- [ ] Write 20+ unit tests for actions
- [ ] Write 20+ unit tests for roles
- [ ] Add ARCHITECTURE.md
- [ ] Fix async optimization
- [ ] Add security validations

### Phase 3 (Nice-to-have)
- [ ] Add 20+ more tests (90% coverage target)
- [ ] Performance profiling & optimization
- [ ] API documentation
- [ ] Example notebooks
- [ ] CI/CD setup (GitHub Actions)
- [ ] WebUI prototype

---

**Updated:** 2024  
**Status:** Ready for implementation  
**Estimated Effort:** 60-80 hours total
