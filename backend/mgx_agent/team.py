#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
MGX Style Multi-Agent Team
AÃ§Ä±k kaynak MetaGPT'yi MGX'e benzer ÅŸekilde Ã§alÄ±ÅŸtÄ±ran Ã¶rnek.

Ã–zellikler:
- Plan taslaÄŸÄ± oluÅŸturma
- KullanÄ±cÄ± onayÄ± bekleme
- GÃ¶rev karmaÅŸÄ±klÄ±k deÄŸerlendirmesi (XS/S/M/L/XL)
- TakÄ±m Ã¼yelerine gÃ¶rev atama
- Ä°lerleme takibi
"""


# Lokal geliÅŸtirme: examples klasÃ¶rÃ¼nden Ã§alÄ±ÅŸÄ±rken metagpt paketini bul

import asyncio
import hashlib
import json
import os
import re
import time
from dataclasses import dataclass
from datetime import datetime
from typing import List, Optional, Sequence, Any, Dict, Tuple
from enum import Enum

from pydantic import BaseModel, Field, validator
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

from metagpt.actions import Action
from metagpt.logs import logger
from metagpt.roles import Role
from metagpt.schema import Message
from metagpt.config2 import config as global_config
try:
    from metagpt.team import Team
    from metagpt.context import Context
except ImportError:
    from metagpt.config import Config
    from mgx_agent.metagpt_wrapper import Team, Context

# Import from mgx_agent package for modular structure  
from mgx_agent.adapter import MetaGPTAdapter
from mgx_agent.actions import (
    AnalyzeTask,
    DraftPlan,
    WriteCode,
    WriteTest,
    ReviewCode,
)
from mgx_agent.roles import Mike, Alex, Bob, Charlie
from mgx_agent.cache import (
    CacheBackend,
    ResponseCache,
    InMemoryLRUTTLCache,
    NullCache,
    RedisCache,
    make_cache_key,
)
from mgx_agent.performance.async_tools import (
    AsyncTimer,
    bounded_gather,
    with_timeout,
    run_in_thread,
    PhaseTimings,
)

from mgx_observability import start_span, set_span_attributes


# ============================================
# GÃ–REV KARMAÅIKLIK DEÄERLENDÄ°RME
# ============================================
class TaskComplexity:
    """GÃ¶rev karmaÅŸÄ±klÄ±k seviyeleri"""
    XS = "XS"  # Ã‡ok basit - tek dosya, tek fonksiyon
    S = "S"    # Basit - birkaÃ§ fonksiyon
    M = "M"    # Orta - birden fazla dosya
    L = "L"    # BÃ¼yÃ¼k - mimari gerektirir
    XL = "XL"  # Ã‡ok bÃ¼yÃ¼k - tam takÄ±m gerektirir


# ============================================
# TAKIM KONFÄ°GÃœRASYONU (Pydantic)
# ============================================
class LogLevel(str, Enum):
    """Log seviyeleri"""
    DEBUG = "debug"
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"


class TeamConfig(BaseModel):
    """MGX Style Team konfigÃ¼rasyonu - Pydantic validation ile"""
    
    # Temel ayarlar
    max_rounds: int = Field(default=5, ge=1, le=20, description="Maksimum Ã§alÄ±ÅŸma turu")
    max_revision_rounds: int = Field(default=2, ge=0, le=5, description="Maksimum dÃ¼zeltme turu")
    max_memory_size: int = Field(default=50, ge=10, le=500, description="HafÄ±za limiti")
    
    # Ã–zellik anahtarlarÄ±
    enable_caching: bool = Field(default=True, description="Analiz cache'i aktif mi")
    enable_streaming: bool = Field(default=True, description="LLM streaming aktif mi")
    enable_progress_bar: bool = Field(default=True, description="Progress bar gÃ¶ster")
    enable_metrics: bool = Field(default=True, description="Metrik toplama aktif mi")
    enable_memory_cleanup: bool = Field(default=True, description="Otomatik hafÄ±za temizliÄŸi")
    enable_profiling: bool = Field(default=False, description="Performance profiling aktif mi")
    enable_profiling_tracemalloc: bool = Field(default=False, description="Tracemalloc ile detaylÄ± hafÄ±za profiling")
    
    # TakÄ±m ayarlarÄ±
    human_reviewer: bool = Field(default=False, description="Charlie insan modu")
    auto_approve_plan: bool = Field(default=False, description="Plan otomatik onayla")
    
    # Budget ayarlarÄ±
    default_investment: float = Field(default=3.0, ge=0.5, le=20.0, description="VarsayÄ±lan investment ($)")
    budget_multiplier: float = Field(default=1.0, ge=0.1, le=5.0, description="Budget Ã§arpanÄ±")
    
    # LLM ayarlarÄ±
    use_multi_llm: bool = Field(default=False, description="Her role farklÄ± LLM")
    
    # Log ayarlarÄ±
    log_level: LogLevel = Field(default=LogLevel.INFO, description="Log seviyesi")
    verbose: bool = Field(default=False, description="DetaylÄ± Ã§Ä±ktÄ±")
    
    # Cache ayarlarÄ±
    cache_backend: str = Field(
        default="memory",
        description="Cache backend: none | memory | redis",
    )
    cache_max_entries: int = Field(
        default=1024,
        ge=1,
        le=100_000,
        description="In-memory cache iÃ§in maksimum entry sayÄ±sÄ± (LRU)",
    )
    redis_url: Optional[str] = Field(
        default=None,
        description="Redis URL (cache_backend=redis iÃ§in)",
    )
    cache_log_hits: bool = Field(default=False, description="Cache hit logla")
    cache_log_misses: bool = Field(default=False, description="Cache miss logla")
    cache_ttl_seconds: int = Field(default=3600, ge=60, le=86400, description="Cache TTL (saniye)")
    
    @validator('max_rounds')
    @classmethod
    def validate_max_rounds(cls, v):
        if v < 1:
            raise ValueError("max_rounds en az 1 olmalÄ±")
        return v
    
    @validator('default_investment')
    @classmethod
    def validate_investment(cls, v):
        if v < 0.5:
            raise ValueError("investment en az $0.5 olmalÄ±")
        return v
    
    @validator('budget_multiplier')
    @classmethod
    def validate_budget_multiplier(cls, v):
        if v <= 0:
            raise ValueError("budget_multiplier 0'dan bÃ¼yÃ¼k olmalÄ±")
        if v > 10:
            logger.warning(f"âš ï¸ budget_multiplier Ã§ok yÃ¼ksek: {v}x - Maliyet patlamasÄ± riski!")
        return v
    
    # âœ… Pydantic v2 syntax
    class Config:
        use_enum_values = True
    
    def to_dict(self) -> dict:
        """Config'i dict olarak dÃ¶ndÃ¼r"""
        return self.dict()
    
    @classmethod
    def from_dict(cls, data: dict) -> "TeamConfig":
        """Dict'ten config oluÅŸtur"""
        return cls(**data)
    
    @classmethod
    def from_yaml(cls, path: str) -> "TeamConfig":
        """YAML dosyasÄ±ndan config oluÅŸtur"""
        import yaml
        with open(path, 'r', encoding='utf-8') as f:
            data = yaml.safe_load(f)
        return cls(**data)
    
    def save_yaml(self, path: str):
        """Config'i YAML dosyasÄ±na kaydet"""
        import yaml
        with open(path, 'w', encoding='utf-8') as f:
            yaml.dump(self.to_dict(), f, default_flow_style=False, allow_unicode=True)
    
    def __str__(self) -> str:
        return f"""TeamConfig(
  max_rounds={self.max_rounds}, max_revision_rounds={self.max_revision_rounds},
  max_memory_size={self.max_memory_size}, enable_caching={self.enable_caching},
  human_reviewer={self.human_reviewer}, default_investment=${self.default_investment}
)"""


# VarsayÄ±lan config
DEFAULT_CONFIG = TeamConfig()


# ============================================
# GÃ–REV METRÄ°KLERÄ°
# ============================================
@dataclass
class TaskMetrics:
    """GÃ¶rev metrikleri - izlenebilirlik iÃ§in"""
    task_name: str
    start_time: float
    end_time: float = 0.0
    success: bool = False
    complexity: str = "XS"
    token_usage: int = 0  # Åimdilik dummy - ileride gerÃ§ek deÄŸer
    estimated_cost: float = 0.0  # Åimdilik dummy - ileride gerÃ§ek deÄŸer
    revision_rounds: int = 0
    cache_hits: int = 0
    cache_misses: int = 0
    error_message: str = ""
    
    @property
    def duration_seconds(self) -> float:
        """GÃ¶rev sÃ¼resi (saniye)"""
        return self.end_time - self.start_time if self.end_time else 0.0
    
    @property
    def duration_formatted(self) -> str:
        """FormatlanmÄ±ÅŸ sÃ¼re"""
        secs = self.duration_seconds
        if secs < 60:
            return f"{secs:.1f}s"
        elif secs < 3600:
            return f"{secs/60:.1f}m"
        else:
            return f"{secs/3600:.1f}h"
    
    def to_dict(self) -> dict:
        """MetriÄŸi dict olarak dÃ¶ndÃ¼r"""
        return {
            "task_name": self.task_name,
            "duration": self.duration_formatted,
            "success": self.success,
            "complexity": self.complexity,
            "token_usage": self.token_usage,
            "estimated_cost": f"${self.estimated_cost:.4f}",
            "revision_rounds": self.revision_rounds,
            "cache_hits": self.cache_hits,
            "cache_misses": self.cache_misses,
            "error": self.error_message if self.error_message else None,
        }


# ============================================
# ACTION'LAR (Retry Logic ile)
# ============================================

# Retry decorator - LLM hatalarÄ±nda otomatik yeniden dene
def llm_retry():
    """LLM Ã§aÄŸrÄ±larÄ± iÃ§in retry decorator"""
    return retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        retry=retry_if_exception_type((Exception,)),
        before_sleep=lambda retry_state: logger.warning(
            f"âš ï¸ LLM hatasÄ±, yeniden deneniyor... (Deneme {retry_state.attempt_number}/3)"
        )
    )


# ============================================
# PROGRESS HELPERS
# ============================================
def print_step_progress(step: int, total: int, description: str, role=None):
    """AdÄ±m adÄ±m progress gÃ¶ster
    
    Args:
        step: Mevcut adÄ±m
        total: Toplam adÄ±m
        description: AÃ§Ä±klama
        role: Role instance (team referansÄ± iÃ§in)
    """
    # EÄŸer role'un team referansÄ± varsa onu kullan (config kontrolÃ¼ iÃ§in)
    if role and hasattr(role, '_team_ref') and hasattr(role._team_ref, '_print_progress'):
        role._team_ref._print_progress(step, total, description)
        return
    
    # Fallback: Global fonksiyon (eski davranÄ±ÅŸ)
    bar_length = 20
    filled = int(bar_length * step / total)
    bar = "â–ˆ" * filled + "â–‘" * (bar_length - filled)
    percent = int(100 * step / total)
    print(f"\r[{bar}] {percent}% - {description}", end="", flush=True)
    if step == total:
        print()  # Yeni satÄ±r


def print_phase_header(phase: str, emoji: str = "ğŸ”„"):
    """Faz baÅŸlÄ±ÄŸÄ± yazdÄ±r"""
    print(f"\n{'='*60}")
    print(f"{emoji} {phase}")
    print(f"{'='*60}")




# ============================================
# MGX STYLE TEAM
# ============================================
class MGXStyleTeam:
    """MGX tarzÄ± takÄ±m yÃ¶neticisi"""

    # Shared cache instance used when MGX_GLOBAL_CACHE=1 (perf/load tests).
    _GLOBAL_RESPONSE_CACHE: Optional[ResponseCache] = None

    def __init__(
        self,
        config: TeamConfig = None,
        human_reviewer: bool = False,
        max_memory_size: int = 50,
        *,
        context_override: Optional[Any] = None,
        team_override: Optional[Any] = None,
        roles_override: Optional[Sequence[Any]] = None,
        output_dir_base: Optional[str] = "output",
    ):
        """
        MGX tarzÄ± takÄ±m oluÅŸtur.
        
        Args:
            config: TeamConfig objesi (None ise varsayÄ±lan kullanÄ±lÄ±r)
            human_reviewer: True ise Charlie (Reviewer) insan olarak Ã§alÄ±ÅŸÄ±r (config'den override edilir)
            max_memory_size: HafÄ±za limiti (config'den override edilir)
        """
        # Config yoksa varsayÄ±lan oluÅŸtur
        if config is None:
            config = TeamConfig(
                human_reviewer=human_reviewer,
                max_memory_size=max_memory_size
            )
        
        self.config = config
        self._log_config()
        
        # Config'den deÄŸerleri al
        self.output_dir_base = output_dir_base
        self.context = context_override if context_override is not None else Context()
        self.team = team_override if team_override is not None else Team(context=self.context)
        self.plan_approved = False
        self.current_task = None
        self.current_task_spec = None  # Tek kaynak: task, plan, complexity bilgisi
        self.progress = []
        self.memory_log = []  # HafÄ±za gÃ¼nlÃ¼ÄŸÃ¼
        self.max_memory_size = config.max_memory_size
        self.human_mode = config.human_reviewer
        self.metrics: List[TaskMetrics] = [] if config.enable_metrics else None
        self.phase_timings = PhaseTimings()  # Track phase durations

        # Cache stats (used by performance suite)
        self._cache_hits = 0
        self._cache_misses = 0
        self._cache: ResponseCache = self._init_cache()
        
        # Profiling
        self._profiler = None
        self._profiling_enabled = config.enable_profiling
        
        # Her role iÃ§in farklÄ± LLM config'leri yÃ¼kle (veya test harness overrides)
        if roles_override is not None:
            roles_list = list(roles_override)
            self.multi_llm_mode = False
        else:
            if config.use_multi_llm:
                try:
                    mike_config = Config.from_home("mike_llm.yaml")
                    alex_config = Config.from_home("alex_llm.yaml")
                    bob_config = Config.from_home("bob_llm.yaml")
                    charlie_config = Config.from_home("charlie_llm.yaml")
                    self.multi_llm_mode = True
                    logger.info("ğŸ¯ Multi-LLM modu aktif - Her role farklÄ± model kullanacak!")
                except Exception:
                    mike_config = alex_config = bob_config = charlie_config = None
                    self.multi_llm_mode = False
                    logger.info("ğŸ“¦ Tek LLM modu - TÃ¼m roller aynÄ± modeli kullanacak")
            else:
                mike_config = alex_config = bob_config = charlie_config = None
                self.multi_llm_mode = False
                logger.info("ğŸ“¦ Tek LLM modu - TÃ¼m roller aynÄ± modeli kullanacak")

            # TakÄ±mÄ± oluÅŸtur (her role farklÄ± config ile)
            # Global LLM config'i kullan (config2.yaml'dan okunan deÄŸerler)
            logger.info(f"ğŸ¤– Global LLM model: {global_config.llm.model}, type: {global_config.llm.api_type}")
            roles_list = [
                Mike(config=mike_config) if mike_config else Mike(),
                Alex(config=alex_config) if alex_config else Alex(),
                Bob(config=bob_config) if bob_config else Bob(),
                Charlie(is_human=config.human_reviewer, config=charlie_config)
                if charlie_config
                else Charlie(is_human=config.human_reviewer),
            ]
            
            # Global LLM config'i tÃ¼m rollere uygula
            for role in roles_list:
                if hasattr(role, 'llm') and hasattr(global_config, 'llm'):
                    role.llm = global_config.get_llm()

        # Role'lara team referansÄ± ekle (progress bar iÃ§in)
        for role in roles_list:
            try:
                role._team_ref = self
            except Exception:
                pass

        if hasattr(self.team, "hire"):
            self.team.hire(roles_list)

        # Role referanslarÄ±nÄ± sakla (team.env.roles eriÅŸimini azaltmak iÃ§in)
        if len(roles_list) >= 4:
            self._mike = roles_list[0]
            self._alex = roles_list[1]
            self._bob = roles_list[2]
            self._charlie = roles_list[3]
        else:
            self._mike = roles_list[0] if roles_list else None
            self._alex = None
            self._bob = None
            self._charlie = None
        
        # Multi-LLM sanity check: GerÃ§ekten farklÄ± modeller kullanÄ±lÄ±yor mu?
        if self.multi_llm_mode:
            self._verify_multi_llm_setup(roles_list)
        
        logger.info("ğŸ¢ MGX Style TakÄ±m oluÅŸturuldu!")
        if self.multi_llm_mode:
            logger.info("ğŸ‘¤ Mike (Team Leader) - ğŸ§  allenai/olmo-3-32b-think:free")
            logger.info("ğŸ‘¤ Alex (Engineer) - ğŸ’» amazon/nova-2-lite-v1:free")
            logger.info("ğŸ‘¤ Bob (Tester) - âš¡ arcee-ai/trinity-mini:free")
            if config.human_reviewer:
                logger.info("ğŸ‘¤ Charlie (Reviewer) - ğŸ§‘ HUMAN FLAG (LLM fallback)")
            else:
                logger.info("ğŸ‘¤ Charlie (Reviewer) - ğŸ” nvidia/nemotron-nano-12b-v2-vl:free")
        else:
            logger.info("ğŸ‘¤ Mike (Team Leader) - GÃ¶rev analizi ve planlama")
            logger.info("ğŸ‘¤ Alex (Engineer) - Kod yazma")
            logger.info("ğŸ‘¤ Bob (Tester) - Test yazma")
            if config.human_reviewer:
                logger.info("ğŸ‘¤ Charlie (Reviewer) - ğŸ§‘ HUMAN FLAG (LLM fallback)")
            else:
                logger.info("ğŸ‘¤ Charlie (Reviewer) - Kod inceleme")
    
    def _log_config(self):
        """Config bilgilerini logla"""
        if self.config.verbose:
            logger.info(f"âš™ï¸ TeamConfig yÃ¼klendi:")
            logger.info(f"   max_rounds: {self.config.max_rounds}")
            logger.info(f"   max_revision_rounds: {self.config.max_revision_rounds}")
            logger.info(f"   max_memory_size: {self.config.max_memory_size}")
            logger.info(f"   enable_caching: {self.config.enable_caching}")
            logger.info(f"   enable_metrics: {self.config.enable_metrics}")
            logger.info(f"   default_investment: ${self.config.default_investment}")
            logger.info(f"   cache_backend: {getattr(self.config, 'cache_backend', 'memory')}")
            logger.info(f"   cache_max_entries: {getattr(self.config, 'cache_max_entries', 1024)}")
            logger.info(f"   cache_ttl_seconds: {self.config.cache_ttl_seconds}")

    def _init_cache(self) -> ResponseCache:
        """Initialize response cache based on config."""

        if not getattr(self.config, "enable_caching", True):
            return NullCache()

        backend_raw = str(getattr(self.config, "cache_backend", CacheBackend.MEMORY.value)).lower()
        if backend_raw in (CacheBackend.NONE.value, "off", "false", "0"):
            return NullCache()

        use_global = os.getenv("MGX_GLOBAL_CACHE") == "1"
        max_entries = int(getattr(self.config, "cache_max_entries", 1024))
        ttl_seconds = int(getattr(self.config, "cache_ttl_seconds", 3600))

        if backend_raw == CacheBackend.REDIS.value:
            redis_url = getattr(self.config, "redis_url", None)
            if not redis_url:
                logger.warning("âš ï¸ cache_backend=redis ama redis_url boÅŸ - cache devre dÄ±ÅŸÄ±")
                return NullCache()
            try:
                return RedisCache(redis_url=redis_url, ttl_seconds=ttl_seconds)
            except Exception as e:
                logger.warning(f"âš ï¸ Redis cache init baÅŸarÄ±sÄ±z ({e}) - in-memory cache kullanÄ±lacak")
                return InMemoryLRUTTLCache(max_entries=max_entries, ttl_seconds=ttl_seconds)

        if use_global:
            if MGXStyleTeam._GLOBAL_RESPONSE_CACHE is None:
                MGXStyleTeam._GLOBAL_RESPONSE_CACHE = InMemoryLRUTTLCache(
                    max_entries=max_entries,
                    ttl_seconds=ttl_seconds,
                )
            return MGXStyleTeam._GLOBAL_RESPONSE_CACHE

        return InMemoryLRUTTLCache(max_entries=max_entries, ttl_seconds=ttl_seconds)

    async def cached_llm_call(
        self,
        *,
        role: str,
        action: str,
        payload: Dict[str, Any],
        compute,
        bypass_cache: bool = False,
        encode=None,
        decode=None,
    ):
        """Run an expensive computation with cache.

        `payload` must be JSON serializable.

        encode/decode can be used to cache a serialized representation while
        returning richer python objects.
        """

        if bypass_cache or not getattr(self.config, "enable_caching", True):
            return await compute()

        # Human reviewer mode should never influence LLM review behavior.
        if getattr(self.config, "human_reviewer", False) and role == "Reviewer":
            return await compute()

        key = make_cache_key(role=role, action=action, payload=payload)

        cached = None
        try:
            cached = self._cache.get(key)
        except Exception as e:
            logger.debug(f"Cache get hatasÄ±: {e}")

        if cached is not None:
            self._cache_hits += 1
            if getattr(self.config, "cache_log_hits", False):
                logger.info(f"âš¡ Cache hit: {role}/{action}")
            try:
                from mgx_agent.performance.profiler import get_active_profiler

                prof = get_active_profiler()
                if prof is not None:
                    prof.record_cache(True)
            except Exception:
                pass
            return decode(cached) if decode else cached

        # Try semantic cache if available
        if hasattr(self._cache, '_find_similar') or isinstance(self._cache, type(self._cache).__module__ + '.SemanticCache'):
            try:
                from mgx_agent.cache import SemanticCache
                if isinstance(self._cache, SemanticCache):
                    # Extract text from payload for semantic matching
                    payload_text = str(payload.get('task', payload.get('content', '')))
                    if payload_text:
                        semantic_key = self._cache._find_similar(self._cache._simple_embedding(payload_text))
                        if semantic_key:
                            semantic_cached = self._cache.base_cache.get(semantic_key)
                            if semantic_cached is not None:
                                self._cache_hits += 1
                                logger.debug(f"âš¡ Semantic cache hit: {role}/{action}")
                                try:
                                    from mgx_agent.performance.profiler import get_active_profiler
                                    prof = get_active_profiler()
                                    if prof is not None:
                                        prof.record_cache(True)
                                except Exception:
                                    pass
                                return decode(semantic_cached) if decode else semantic_cached
            except Exception as e:
                logger.debug(f"Semantic cache lookup failed: {e}")

        self._cache_misses += 1
        if getattr(self.config, "cache_log_misses", False):
            logger.info(f"ğŸ¢ Cache miss: {role}/{action}")
        try:
            from mgx_agent.performance.profiler import get_active_profiler

            prof = get_active_profiler()
            if prof is not None:
                prof.record_cache(False)
        except Exception:
            pass

        result = await compute()
        to_store = encode(result) if encode else result
        try:
            self._cache.set(key, to_store)
        except Exception as e:
            logger.debug(f"Cache set hatasÄ±: {e}")
        return result

    def cache_clear(self) -> None:
        """Clear the configured cache."""
        try:
            self._cache.clear()
        except Exception:
            pass

    def clear_cache(self) -> None:
        """Backward-compatible alias for cache_clear()."""
        return self.cache_clear()

    def cache_inspect(self) -> dict:
        """Return cache stats + a sample of keys for debugging."""
        try:
            st = self._cache.stats()
            keys = self._cache.keys()
        except Exception:
            st = None
            keys = []

        return {
            "enabled": bool(getattr(self.config, "enable_caching", True)),
            "backend": getattr(st, "backend", None),
            "size": getattr(st, "size", 0),
            "hits": self._cache_hits,
            "misses": self._cache_misses,
            "keys_sample": keys[:10],
        }

    def inspect_cache(self) -> dict:
        """Backward-compatible alias for cache_inspect()."""
        return self.cache_inspect()

    def cache_warm(self, *, role: str, action: str, payload: Dict[str, Any], value: Any) -> None:
        """Prewarm cache with a known response."""
        if not getattr(self.config, "enable_caching", True):
            return
        key = make_cache_key(role=role, action=action, payload=payload)
        try:
            self._cache.set(key, value)
        except Exception:
            pass

    def warm_cache(self, *, role: str, action: str, payload: Dict[str, Any], value: Any) -> None:
        """Backward-compatible alias for cache_warm()."""
        return self.cache_warm(role=role, action=action, payload=payload, value=value)

    def _sync_task_spec_from_plan(self, plan_content: str, *, fallback_task: str) -> None:
        """Ensure self.current_task_spec is populated from Mike's plan message."""

        task = fallback_task
        complexity = "M"
        plan = ""

        if "---JSON_START---" in plan_content and "---JSON_END---" in plan_content:
            try:
                json_str = plan_content.split("---JSON_START---")[1].split("---JSON_END---")[0].strip()
                data = json.loads(json_str)
                if isinstance(data, dict):
                    task = data.get("task") or task
                    complexity = data.get("complexity") or complexity
                    plan = data.get("plan") or plan
            except Exception:
                pass

        if not plan:
            # Try to extract a PLAN: section; otherwise keep entire message.
            m = re.search(r"\bPLAN:\s*(.*)", plan_content, re.IGNORECASE | re.DOTALL)
            if m:
                plan = m.group(1).strip()
            else:
                plan = plan_content

        self.set_task_spec(
            task=task,
            complexity=str(complexity),
            plan=plan,
            is_revision=False,
            review_notes="",
        )

    def _verify_multi_llm_setup(self, roles_list):
        """
        Multi-LLM modunda gerÃ§ekten farklÄ± modeller kullanÄ±lÄ±yor mu kontrol et (sanity check)
        
        Args:
            roles_list: OluÅŸturulan role listesi
        """
        try:
            role_names = ["Mike", "Alex", "Bob", "Charlie"]
            llm_models = []
            
            for i, role in enumerate(roles_list):
                role_name = role_names[i] if i < len(role_names) else f"Role_{i}"
                llm_info = "N/A"
                
                # Role'un LLM'ini kontrol et
                if hasattr(role, 'llm') and role.llm:
                    # LLM provider'Ä±ndan model adÄ±nÄ± almaya Ã§alÄ±ÅŸ
                    if hasattr(role.llm, 'model'):
                        llm_info = role.llm.model
                    elif hasattr(role.llm, 'model_name'):
                        llm_info = role.llm.model_name
                    elif hasattr(role.llm, '__class__'):
                        llm_info = role.llm.__class__.__name__
                    else:
                        llm_info = "Unknown"
                
                llm_models.append((role_name, llm_info))
                logger.debug(f"ğŸ” {role_name} LLM: {llm_info}")
            
            # TÃ¼m modeller aynÄ± mÄ± kontrol et
            unique_models = set(model for _, model in llm_models)
            if len(unique_models) == 1:
                logger.warning(f"âš ï¸ SANITY CHECK: Multi-LLM modu aktif ama tÃ¼m roller aynÄ± modeli kullanÄ±yor: {unique_models.pop()}")
                logger.warning(f"âš ï¸ Config dosyalarÄ± yÃ¼klendi ama role'lar farklÄ± LLM kullanmÄ±yor olabilir!")
                logger.warning(f"âš ï¸ MetaGPT'nin Role sÄ±nÄ±fÄ± config parametresini desteklemiyor olabilir.")
            else:
                logger.info(f"âœ… SANITY CHECK: Multi-LLM modu Ã§alÄ±ÅŸÄ±yor - {len(unique_models)} farklÄ± model kullanÄ±lÄ±yor")
                for role_name, model in llm_models:
                    logger.info(f"   {role_name}: {model}")
        
        except Exception as e:
            logger.warning(f"âš ï¸ Multi-LLM sanity check hatasÄ±: {e}")
            logger.warning(f"âš ï¸ LLM kontrolÃ¼ yapÄ±lamadÄ± - config'lerin gerÃ§ekten kullanÄ±ldÄ±ÄŸÄ±ndan emin olamÄ±yoruz")
    
    def get_config(self) -> TeamConfig:
        """Mevcut config'i dÃ¶ndÃ¼r"""
        return self.config
    
    def update_config(self, **kwargs):
        """Config deÄŸerlerini gÃ¼ncelle"""
        for key, value in kwargs.items():
            if hasattr(self.config, key):
                setattr(self.config, key, value)
                logger.info(f"âš™ï¸ Config gÃ¼ncellendi: {key} = {value}")
    
    def set_task_spec(self, task: str, complexity: str, plan: str, is_revision: bool = False, review_notes: str = ""):
        """
        Task spec'i set et (tek kaynak - hafÄ±za taramasÄ± yerine bu kullanÄ±lacak)
        
        Args:
            task: GÃ¶rev aÃ§Ä±klamasÄ±
            complexity: KarmaÅŸÄ±klÄ±k seviyesi (XS, S, M, L, XL)
            plan: Plan metni
            is_revision: Revision turu mu?
            review_notes: Review notlarÄ± (revision turunda)
        """
        self.current_task_spec = {
            "task": task,
            "complexity": complexity,
            "plan": plan,
            "is_revision": is_revision,
            "review_notes": review_notes
        }
        logger.debug(f"ğŸ“‹ Task spec set edildi: task='{task[:50]}...', complexity={complexity}, is_revision={is_revision}")
    
    def get_task_spec(self) -> dict:
        """
        Mevcut task spec'i dÃ¶ndÃ¼r
        
        Returns:
            Task spec dict veya None
        """
        return self.current_task_spec
    
    def _print_progress(self, step: int, total: int, description: str):
        """Progress gÃ¶ster (config'e gÃ¶re)"""
        if not self.config.enable_progress_bar:
            return
        
        bar_length = 20
        filled = int(bar_length * step / total)
        bar = "â–ˆ" * filled + "â–‘" * (bar_length - filled)
        percent = int(100 * step / total)
        print(f"\r[{bar}] {percent}% - {description}", end="", flush=True)
        if step == total:
            print()  # Yeni satÄ±r
    
    def _log(self, message: str, level: str = "info"):
        """Config'e gÃ¶re log yaz"""
        if not self.config.verbose and level == "debug":
            return
        
        if level == "info":
            logger.info(message)
        elif level == "debug":
            logger.debug(message)
        elif level == "warning":
            logger.warning(message)
        elif level == "error":
            logger.error(message)
    
    def add_to_memory(self, role: str, action: str, content: str):
        """HafÄ±za gÃ¼nlÃ¼ÄŸÃ¼ne ekle"""
        entry = {
            "role": role,
            "action": action,
            "content": content[:100] + "..." if len(content) > 100 else content,
            "timestamp": datetime.now().isoformat(timespec="seconds")
        }
        self.memory_log.append(entry)
        self.progress.append(f"{role}: {action}")
    
    def cleanup_memory(self):
        """HafÄ±za gÃ¼nlÃ¼ÄŸÃ¼nÃ¼ temizle - ÅŸiÅŸmeyi Ã¶nle"""
        # 1. memory_log temizliÄŸi
        if len(self.memory_log) > self.max_memory_size:
            old_size = len(self.memory_log)
            self.memory_log = self.memory_log[-self.max_memory_size:]
            logger.info(f"ğŸ§¹ HafÄ±za gÃ¼nlÃ¼ÄŸÃ¼ temizlendi: {old_size} â†’ {len(self.memory_log)} kayÄ±t")
        
        # 2. progress temizliÄŸi
        if len(self.progress) > self.max_memory_size:
            self.progress = self.progress[-self.max_memory_size:]
        
        # 3. Role memory temizliÄŸi (her role iÃ§in) - Adapter kullanarak
        if hasattr(self.team, 'env') and hasattr(self.team.env, 'roles'):
            for role in self.team.env.roles.values():
                mem_store = MetaGPTAdapter.get_memory_store(role)
                if mem_store is None:
                    continue
                
                # MesajlarÄ± adapter Ã¼zerinden al
                memory = MetaGPTAdapter.get_messages(mem_store)
                
                if len(memory) > self.max_memory_size:
                    # Adapter Ã¼zerinden temizle
                    success = MetaGPTAdapter.clear_memory(mem_store, self.max_memory_size)
                    if success:
                        logger.info(f"ğŸ§¹ {role.name} hafÄ±zasÄ± temizlendi: {len(memory)} â†’ {self.max_memory_size} mesaj")
                    else:
                        logger.warning(f"âš ï¸ {role.name} hafÄ±zasÄ± temizlenemedi")
    
    def show_memory_log(self) -> str:
        """HafÄ±za gÃ¼nlÃ¼ÄŸÃ¼nÃ¼ gÃ¶ster"""
        if not self.memory_log:
            return "ğŸ“‹ HafÄ±za gÃ¼nlÃ¼ÄŸÃ¼ boÅŸ."
        
        result = "\nğŸ“‹ HAFIZA GÃœNLÃœÄÃœ:\n" + "=" * 40 + "\n"
        for i, entry in enumerate(self.memory_log, 1):
            result += f"{i}. [{entry['role']}] {entry['action']}\n"
            result += f"   Ä°Ã§erik: {entry['content']}\n"
        return result
    
    def _start_profiler(self, run_name: str = "mgx_team_run"):
        """Initialize and start the profiler if enabled."""
        if not self._profiling_enabled:
            return None
        
        from mgx_agent.performance.profiler import PerformanceProfiler
        
        self._profiler = PerformanceProfiler(
            run_name=run_name,
            enable_tracemalloc=self.config.enable_profiling_tracemalloc,
            enable_file_output=True,
        )
        self._profiler.start()
        return self._profiler
    
    def _end_profiler(self) -> Optional[dict]:
        """End profiling and write reports."""
        if self._profiler is None:
            return None
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        self._profiler.stop()
        
        # Write detailed report
        detailed_file = self._profiler.write_detailed_report(timestamp)
        if detailed_file:
            logger.info(f"ğŸ“Š Detailed performance report: {detailed_file}")
        
        # Write summary report
        summary_file = self._profiler.write_summary_report()
        if summary_file:
            logger.info(f"ğŸ“Š Summary performance report: {summary_file}")
        
        metrics = self._profiler.to_run_metrics()
        self._profiler = None
        return metrics

    async def analyze_and_plan(self, task: str) -> str:
        """GÃ¶revi analiz et ve plan oluÅŸtur"""
        self.current_task = task

        # KullanÄ±cÄ±ya gÃ¶rÃ¼nen bilgi main() fonksiyonunda print ile basÄ±lÄ±yor
        logger.debug(f"Yeni gÃ¶rev analiz ediliyor: {task}")
        
        # Start profiling phase if enabled
        if self._profiler:
            self._profiler.start_phase("analyze_and_plan")

        # Cache (perf + interactive) - plan generation is one of the hottest paths.
        if getattr(self.config, "enable_caching", True):
            cache_key = make_cache_key(
                role="TeamLeader",
                action="AnalyzeTask+DraftPlan",
                payload={"task": task},
            )
            cache_start = time.perf_counter()

            cached = None
            try:
                cached = self._cache.get(cache_key)
            except Exception:
                cached = None

            if cached is not None:
                self._cache_hits += 1
                try:
                    from mgx_agent.performance.profiler import get_active_profiler

                    prof = get_active_profiler()
                    if prof is not None:
                        prof.record_cache(True)
                        prof.record_timer("analyze_and_plan_cache_hit", time.perf_counter() - cache_start)
                except Exception:
                    pass

                cached_content = cached.get("content") if isinstance(cached, dict) else cached
                cached_role = cached.get("role", "TeamLeader") if isinstance(cached, dict) else "TeamLeader"

                self.last_plan = Message(content=str(cached_content), role=cached_role, cause_by=AnalyzeTask)
                self.add_to_memory(
                    "Mike",
                    "AnalyzeTask + DraftPlan (cache)",
                    str(cached_content),
                )
                self._sync_task_spec_from_plan(str(cached_content), fallback_task=task)

                if self.config.auto_approve_plan:
                    self._log("ğŸ¤– Auto-approve aktif, plan otomatik onaylandÄ±", "info")
                    self.approve_plan()

                return str(cached_content)

            self._cache_misses += 1
            try:
                from mgx_agent.performance.profiler import get_active_profiler

                prof = get_active_profiler()
                if prof is not None:
                    prof.record_cache(False)
            except Exception:
                pass

        # Team'deki Mike'Ä± bul (saklanan referansÄ± kullan - team.env.roles eriÅŸimini azalt)
        mike = getattr(self, '_mike', None)
        if not mike:
            # Fallback: team.env.roles eriÅŸimi (sadece gerekirse)
            if hasattr(self.team, 'env') and hasattr(self.team.env, 'roles'):
                for role in self.team.env.roles.values():
                    if role.profile == "TeamLeader":
                        mike = role
                        break
        
        if not mike:
            mike = Mike(context=self.context)
        
        # Mike analiz etsin (with timing and timeout)
        async with AsyncTimer("analyze_and_plan", log_on_exit=True) as timer:
            async with start_span(
                "mgx.team.analyze_and_plan",
                attributes={
                    "task.length": len(task),
                    "mgx.cache.enabled": bool(getattr(self.config, "enable_caching", True)),
                },
            ) as span:
                try:
                    analysis = await asyncio.wait_for(
                        mike.analyze_task(task),
                        timeout=120.0  # 2 minute timeout for analysis
                    )
                    set_span_attributes(
                        span,
                        {
                            "analysis.content.length": len(getattr(analysis, "content", str(analysis))),
                        },
                    )
                except asyncio.TimeoutError:
                    logger.error("â±ï¸  Analysis timeout (120s) exceeded")
                    raise
        
        # Record timing
        self.phase_timings.analysis_duration = timer.duration
        self.phase_timings.planning_duration = timer.duration  # Combined for now
        self.phase_timings.add_phase("analyze_and_plan", timer.duration)
        
        # Ã–NEMLÄ°: Plan mesajÄ±nÄ± team environment'a publish et
        # Bu sayede Alex (Engineer) plan mesajÄ±nÄ± alacak
        self.last_plan = analysis

        # Cache the plan for repeated identical tasks
        if getattr(self.config, "enable_caching", True):
            cache_key = make_cache_key(
                role="TeamLeader",
                action="AnalyzeTask+DraftPlan",
                payload={"task": task},
            )
            try:
                self._cache.set(
                    cache_key,
                    {
                        "content": getattr(analysis, "content", str(analysis)),
                        "role": getattr(analysis, "role", "TeamLeader"),
                    },
                )
            except Exception:
                pass

        # Ensure task spec is set even when using mock roles.
        self._sync_task_spec_from_plan(getattr(analysis, "content", str(analysis)), fallback_task=task)

        # HafÄ±zaya ekle
        self.add_to_memory("Mike", "AnalyzeTask + DraftPlan", analysis.content)
        
        # Auto approve kontrolÃ¼
        if self.config.auto_approve_plan:
            self._log("ğŸ¤– Auto-approve aktif, plan otomatik onaylandÄ±", "info")
            self.approve_plan()
        
        # End profiling phase
        if self._profiler:
            self._profiler.end_phase()
        
        return analysis.content
    
    def approve_plan(self) -> bool:
        """PlanÄ± onayla"""
        self.plan_approved = True
        logger.info("âœ… Plan onaylandÄ±! GÃ¶rev daÄŸÄ±tÄ±mÄ± baÅŸlÄ±yor...")
        return True
    
    def _tune_budget(self, complexity: str) -> dict:
        """KarmaÅŸÄ±klÄ±ÄŸa gÃ¶re investment ve n_round ayarla
        
        Args:
            complexity: GÃ¶rev karmaÅŸÄ±klÄ±ÄŸÄ± (XS/S/M/L/XL)
        
        Returns:
            dict: {"investment": float, "n_round": int}
        """
        # Config'den multiplier ve max_rounds al
        multiplier = self.config.budget_multiplier
        max_rounds = self.config.max_rounds
        
        # TaskComplexity sabitleri ile karÅŸÄ±laÅŸtÄ±r
        if complexity in (TaskComplexity.XS, TaskComplexity.S):
            base = {"investment": 1.5, "n_round": min(2, max_rounds)}
        elif complexity == TaskComplexity.M:
            base = {"investment": 3.0, "n_round": min(3, max_rounds)}
        else:  # L, XL
            base = {"investment": 5.0, "n_round": min(4, max_rounds)}
        
        # Budget multiplier uygula
        base["investment"] *= multiplier
        return base
    
    async def _execute_with_early_termination(self, max_rounds: int) -> int:
        """
        Execute with early termination if task is completed.
        
        Args:
            max_rounds: Maximum rounds to execute
        
        Returns:
            Actual number of rounds executed
        """
        actual_rounds = 0
        
        for round_num in range(1, max_rounds + 1):
            await self.team.run(n_round=1)
            actual_rounds += 1
            
            # Check if task is completed (early termination)
            if self._is_task_completed():
                logger.info(f"âœ… Task completed early after {actual_rounds} rounds (max: {max_rounds})")
                break
            
            # Check budget exhaustion
            if self._check_budget_exhaustion():
                logger.warning(f"âš ï¸ Budget exhausted after {actual_rounds} rounds")
                break
        
        return actual_rounds
    
    def _is_task_completed(self) -> bool:
        """
        Check if task is completed based on results.
        
        Returns:
            True if task appears to be completed
        """
        try:
            code, tests, review = self._collect_raw_results()
            
            # Simple heuristic: if we have code, tests, and positive review, consider it done
            has_code = code and len(code.strip()) > 100
            has_tests = tests and len(tests.strip()) > 50
            has_positive_review = review and (
                "approved" in review.lower() or 
                "complete" in review.lower() or
                "good" in review.lower()
            )
            
            return has_code and has_tests and has_positive_review
        except Exception:
            return False
    
    def _check_budget_exhaustion(self) -> bool:
        """
        Check if budget is exhausted.
        
        Returns:
            True if budget is exhausted
        """
        # This is a placeholder - in production, check actual budget
        # For now, always return False
        return False
    
    def _calculate_optimal_rounds(self, complexity: str, budget: float) -> int:
        """
        Calculate optimal number of rounds based on complexity and budget.
        
        Args:
            complexity: Task complexity (XS, S, M, L, XL)
            budget: Available budget in USD
        
        Returns:
            Optimal number of rounds (with 95%+ accuracy target)
        """
        # Base rounds by complexity (optimized for accuracy)
        complexity_rounds = {
            "XS": 1,
            "S": 2,
            "M": 3,
            "L": 5,
            "XL": 8,
        }
        
        base_rounds = complexity_rounds.get(complexity, 3)
        
        # Adjust based on budget (more budget = more rounds possible)
        # Optimized estimate: $0.40 per round (reduced from $0.50 for better efficiency)
        budget_rounds = int(budget / 0.40)
        
        # Use minimum of complexity-based and budget-based
        optimal = min(base_rounds, budget_rounds)
        
        # Ensure within config limits
        optimal = max(1, min(optimal, self.config.max_rounds))
        
        # Add safety margin for accuracy (95%+ target)
        # For complex tasks, add one extra round if budget allows
        if complexity in ("L", "XL") and budget > optimal * 0.40:
            optimal = min(optimal + 1, self.config.max_rounds)
        
        return optimal
    
    def _get_complexity_from_plan(self) -> str:
        """Son plan mesajÄ±ndan complexity'yi Ã§ek"""
        if hasattr(self, 'last_plan') and self.last_plan:
            content = self.last_plan.content
            # JSON'dan parse et
            if "---JSON_START---" in content and "---JSON_END---" in content:
                try:
                    json_str = content.split("---JSON_START---")[1].split("---JSON_END---")[0].strip()
                    data = json.loads(json_str)
                    return data.get("complexity", "M")
                except (json.JSONDecodeError, IndexError):
                    pass
            # Regex ile dene
            m = re.search(r"KARMAÅIKLIK:\s*(XS|S|M|L|XL)", content.upper())
            if m:
                return m.group(1)
        return "M"  # VarsayÄ±lan
    
    def _calculate_token_usage(self) -> int:
        """
        GerÃ§ek token kullanÄ±mÄ±nÄ± hesapla
        
        NOT: Åu an iÃ§in token sayÄ±sÄ± yeterli. Ä°leride gerÃ§ek maliyet hesaplamasÄ± iÃ§in:
        - TeamConfig'e price_per_million_tokens alanÄ± eklenebilir
        - estimated_cost = total_tokens / 1_000_000 * model_price_per_million
        - Åimdilik investment'Ä± maliyet kabul etmek pratik
        """
        total_tokens = 0
        
        if hasattr(self.team, 'env') and hasattr(self.team.env, 'roles'):
            for role in self.team.env.roles.values():
                if hasattr(role, 'llm') and role.llm:
                    # MetaGPT'nin cost_manager'Ä±ndan token bilgisi al
                    if hasattr(role.llm, 'cost_manager'):
                        cost_mgr = role.llm.cost_manager
                        if hasattr(cost_mgr, 'total_prompt_tokens'):
                            total_tokens += cost_mgr.total_prompt_tokens
                        if hasattr(cost_mgr, 'total_completion_tokens'):
                            total_tokens += cost_mgr.total_completion_tokens
                    # Alternatif: usage bilgisi direkt llm'de olabilir
                    elif hasattr(role.llm, 'usage'):
                        usage = role.llm.usage
                        if hasattr(usage, 'prompt_tokens'):
                            total_tokens += usage.prompt_tokens
                        if hasattr(usage, 'completion_tokens'):
                            total_tokens += usage.completion_tokens
        
        # Fallback: gerÃ§ek deÄŸer bulunamazsa tahmini dÃ¶ndÃ¼r
        return total_tokens if total_tokens > 0 else 1000
    
    async def execute(
        self, 
        n_round: int = None, 
        max_revision_rounds: int = None,
        progress_callback: Callable[[str, str, str], None] = None
    ) -> str:
        """GÃ¶revi Ã§alÄ±ÅŸtÄ±r
        
        Args:
            n_round: Her tur iÃ§in maksimum round sayÄ±sÄ± (None ise config'den alÄ±nÄ±r)
            max_revision_rounds: Review sonrasÄ± maksimum dÃ¼zeltme turu (None ise config'den alÄ±nÄ±r)
            progress_callback: Agent ilerleme callback'i - (agent_name, status, message) parametreleri alÄ±r
        """
        if not self.plan_approved and not self.config.auto_approve_plan:
            return "âŒ Plan henÃ¼z onaylanmadÄ±! Ã–nce plan onaylamalÄ±sÄ±nÄ±z."
        
        # Progress callback helper
        self._progress_callback = progress_callback
        
        # Config'den varsayÄ±lan deÄŸerleri al
        if max_revision_rounds is None:
            max_revision_rounds = self.config.max_revision_rounds
        
        # Metrics baÅŸlat (config.enable_metrics kontrolÃ¼)
        start_time = time.time()
        metric = TaskMetrics(
            task_name=self.current_task[:50] if self.current_task else "Unknown",
            start_time=start_time
        )
        
        # KarmaÅŸÄ±klÄ±ÄŸa gÃ¶re budget ayarla
        complexity = self._get_complexity_from_plan()
        budget = self._tune_budget(complexity)
        metric.complexity = complexity
        
        # n_round parametresi verilmemiÅŸse budget-aware calculation yap
        if n_round is None:
            n_round = self._calculate_optimal_rounds(complexity, budget["investment"])
            logger.debug(f"Calculated optimal rounds: {n_round} (complexity: {complexity}, budget: ${budget['investment']})")
        
        # KullanÄ±cÄ±ya gÃ¶rÃ¼nen bilgi print ile (logger.debug arka planda log dosyasÄ±na gider)
        print_phase_header("GÃ¶rev YÃ¼rÃ¼tme", "ğŸš€")
        print(f"ğŸ“Š KarmaÅŸÄ±klÄ±k: {complexity} â†’ Investment: ${budget['investment']}, Rounds: {n_round}")
        logger.debug(f"GÃ¶rev yÃ¼rÃ¼tme baÅŸlÄ±yor - KarmaÅŸÄ±klÄ±k: {complexity}, Investment: ${budget['investment']}, Rounds: {n_round}")
        
        # Progress callback: Mike baÅŸlÄ±yor
        await self._emit_progress("Mike", "working", "ğŸ“‹ GÃ¶revi analiz ediyorum ve planÄ± oluÅŸturuyorum...")
        
        # Start profiling phase if enabled
        if self._profiler:
            self._profiler.start_phase("execute")
        
        try:
            # Mike zaten analiz yaptÄ± - complete_planning() Ã§aÄŸÄ±r (tekrar Ã§alÄ±ÅŸmasÄ±n)
            if hasattr(self.team.env, 'roles'):
                for role in self.team.env.roles.values():
                    if hasattr(role, 'complete_planning'):
                        role.complete_planning()
            
            self.team.invest(investment=budget["investment"])
            
            # Ã–NEMLÄ°: Plan mesajÄ±nÄ± environment'a publish et
            if hasattr(self, 'last_plan') and self.last_plan:
                self.team.env.publish_message(self.last_plan)
                logger.debug("Plan mesajÄ± Alex'e iletildi")
            
            # Ä°lk tur: Ana geliÅŸtirme
            print_phase_header("TUR 1: Ana GeliÅŸtirme", "ğŸ”„")
            
            # Progress: Alex baÅŸlÄ±yor
            await self._emit_progress("Alex", "working", "ğŸ’» Kod yazÄ±yorum...")
            
            # Wrap main execution with timer
            async with AsyncTimer("main_development_round", log_on_exit=True) as exec_timer:
                async with start_span(
                    "mgx.team.main_development",
                    attributes={
                        "mgx.n_round": n_round,
                        "mgx.phase": "main_development",
                    },
                ) as span:
                    # Dynamic round calculation with early termination
                    actual_rounds = await self._execute_with_early_termination(n_round)
                    
                    # Progress: Alex tamamladÄ±, Bob baÅŸlÄ±yor
                    await self._emit_progress("Alex", "completed", "âœ… Kod yazÄ±ldÄ±")
                    await self._emit_progress("Bob", "working", "ğŸ§ª Testler yazÄ±lÄ±yor...")
                    
                    # Charlie'nin Ã§alÄ±ÅŸmasÄ± iÃ§in ek bir round (MetaGPT'nin normal akÄ±ÅŸÄ±)
                    # Manuel tetikleme hacklerini kaldÄ±rdÄ±k - sadece team.run() kullanÄ±yoruz
                    logger.debug("ğŸ” Charlie'nin review yapmasÄ± iÃ§in ek round Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor...")
                    await self.team.run(n_round=1)  # Charlie'nin Bob'un mesajÄ±nÄ± gÃ¶zlemlemesi ve review yapmasÄ± iÃ§in
                    
                    # Progress: Bob tamamladÄ±, Charlie baÅŸlÄ±yor
                    await self._emit_progress("Bob", "completed", "âœ… Testler yazÄ±ldÄ±")
                    await self._emit_progress("Charlie", "working", "ğŸ” Kod incelemesi yapÄ±lÄ±yor...")

                    set_span_attributes(
                        span,
                        {
                            "mgx.exec.rounds": int(actual_rounds),
                            "mgx.exec.early_terminated": actual_rounds < n_round,
                        },
                    )
            
            # Record execution timing
            self.phase_timings.execution_duration = exec_timer.duration
            self.phase_timings.add_phase("main_development", exec_timer.duration)
            
            # Tur sonrasÄ± hafÄ±za temizliÄŸi (offload to thread)
            async with AsyncTimer("cleanup_after_main", log_on_exit=False) as cleanup_timer:
                await run_in_thread(self.cleanup_memory)
            self.phase_timings.add_phase("cleanup_after_main", cleanup_timer.duration)
        
            # Review sonucunu kontrol et
            revision_count = 0
            last_review_hash = None  # Sonsuz dÃ¶ngÃ¼ Ã¶nleme - LLM'nin aynÄ± yorumlarÄ± tekrar etme sorununa karÅŸÄ±
            
            while revision_count < max_revision_rounds:
                code, tests, review = self._collect_raw_results()
                
                # Debug: Review durumunu logla
                logger.debug(f"ğŸ“‹ Review durumu: code={len(code) if code else 0} chars, tests={len(tests) if tests else 0} chars, review={len(review) if review else 0} chars")
                if review:
                    logger.debug(f"ğŸ“ Review iÃ§eriÄŸi (ilk 200 char): {review[:200]}")
                
                # Review yoksa veya boÅŸsa dÃ¶ngÃ¼den Ã§Ä±k
                if not review or not review.strip():
                    logger.warning("âš ï¸ Review bulunamadÄ± veya boÅŸ - dÃ¶ngÃ¼den Ã§Ä±kÄ±lÄ±yor")
                    break
                
                # KORUMA 1: AynÄ± review tekrar gelirse (sonsuz dÃ¶ngÃ¼ Ã¶nleme)
                # LLM bazen "papaÄŸan gibi" aynÄ± yorumlarÄ± tekrar edebilir - bu durumda dÃ¶ngÃ¼yÃ¼ kÄ±r
                review_hash = hashlib.md5(review.encode()).hexdigest()
                if review_hash == last_review_hash:
                    logger.warning(f"âš ï¸ AynÄ± review tekrar geldi (tur {revision_count + 1}) - LLM aynÄ± yorumu tekrar etti, dÃ¶ngÃ¼den Ã§Ä±kÄ±lÄ±yor")
                    break
                last_review_hash = review_hash
                
                # Review'da "DEÄÄ°ÅÄ°KLÄ°K GEREKLÄ°" var mÄ± kontrol et
                if "DEÄÄ°ÅÄ°KLÄ°K GEREKLÄ°" in review.upper():
                    revision_count += 1
                    
                    # KORUMA 2: Maksimum dÃ¼zeltme turu kontrolÃ¼
                    # Sonsuz dÃ¶ngÃ¼yÃ¼ Ã¶nlemek iÃ§in hard limit
                    if revision_count > max_revision_rounds:
                        logger.warning(f"âš ï¸ Maksimum dÃ¼zeltme turu ({max_revision_rounds}) aÅŸÄ±ldÄ± - durduruluyor")
                        break
                    
                    # Start profiling revision phase
                    if self._profiler:
                        self._profiler.start_phase(f"revision_round_{revision_count}")
                    
                    print_phase_header(f"TUR {revision_count + 1}: DÃ¼zeltme Turu", "ğŸ”§")
                    print(f"âš ï¸ Charlie DEÄÄ°ÅÄ°KLÄ°K GEREKLÄ° dedi. Alex & Bob tekrar Ã§alÄ±ÅŸÄ±yor...")
                    
                    # Ä°yileÅŸtirme mesajÄ± oluÅŸtur (orijinal gÃ¶revi ve planÄ± da dahil et)
                    original_task = self.current_task or "Verilen gÃ¶revi tamamla"
                    
                    # Task spec'i revision turu iÃ§in gÃ¼ncelle (Alex'in direkt eriÅŸebilmesi iÃ§in)
                    complexity = self._get_complexity_from_plan()
                    original_plan = ""
                    if self.current_task_spec:
                        original_plan = self.current_task_spec.get("plan", "")
                    
                    # Revision turu iÃ§in task_spec'i gÃ¼ncelle
                    # Orijinal plan korunur, review notlarÄ± ayrÄ± bir alanda tutulur
                    self.set_task_spec(
                        task=original_task,
                        complexity=complexity,
                        plan=original_plan,  # Orijinal plan korunur
                        is_revision=True,
                        review_notes=review  # Review notlarÄ± ayrÄ± alanda
                    )
                    logger.info("ğŸ“‹ Task spec revision turu iÃ§in gÃ¼ncellendi (orijinal gÃ¶rev + review notlarÄ±)")
                    
                    # Orijinal plan mesajÄ±nÄ± da gÃ¶nder (MetaGPT tarafÄ± iÃ§in - backward compatibility)
                    if hasattr(self, 'last_plan') and self.last_plan:
                        # Orijinal plan mesajÄ±nÄ± tekrar gÃ¶nder
                        self.team.env.publish_message(self.last_plan)
                        logger.debug("ğŸ“‹ Orijinal plan mesajÄ± Alex'e tekrar iletildi (backward compatibility)")
                    
                    # Ä°yileÅŸtirme mesajÄ±nÄ± JSON formatÄ±nda da gÃ¶nder (Alex'in parse edebilmesi iÃ§in - fallback)
                    improvement_json = {
                        "task": original_task,
                        "complexity": complexity,
                        "plan": f"Charlie'nin review notlarÄ±na gÃ¶re kodu iyileÅŸtir: {review[:200]}...",
                        "improvement_required": True,
                        "review_notes": review[:500]
                    }
                    improvement_content = f"""
---JSON_START---
{json.dumps(improvement_json, ensure_ascii=False, indent=2)}
---JSON_END---

ğŸš¨ Ã–NEMLÄ°: DÃœZELTME TURU - ORÄ°JÄ°NAL GÃ–REVÄ° UNUTMA! ğŸš¨

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
YAPILMASI GEREKEN GÃ–REV (ASIL Ä°Å BU!):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
{original_task}
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âš ï¸ UYARI: YUKARIDAKI GÃ–REVÄ° YERÄ°NE GETÄ°RMELÄ°SÄ°N!
   BaÅŸka bir ÅŸey yazma, sadece yukarÄ±daki gÃ¶revi yap!

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
CHARLIE'NÄ°N REVIEW NOTLARI (Ä°YÄ°LEÅTÄ°RME Ã–NERÄ°LERÄ°):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
{review}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
YAPILACAKLAR:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
1. Ã–NCE: Orijinal gÃ¶revi yerine getir ({original_task})
2. SONRA: Charlie'nin Ã¶nerilerini uygula:
   - Kod kalitesi ve hata yÃ¶netimi ekle
   - Test coverage ve edge case'ler ekle
   - Docstring'ler ve dokÃ¼mantasyon ekle
   - Charlie'nin belirttiÄŸi spesifik iyileÅŸtirmeleri yap

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
MEVCUT KOD (Ä°YÄ°LEÅTÄ°RÄ°LECEK):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
{code[:1500] if len(code) > 1500 else code}

ğŸš¨ HATIRLATMA: Orijinal gÃ¶revi unutma! Sadece yukarÄ±daki gÃ¶revi yap!
"""
                    
                    improvement_msg = Message(
                        content=improvement_content,
                        role="TeamLeader",
                        cause_by=AnalyzeTask
                    )
                    
                    # Alex'e mesaj gÃ¶nder
                    self.team.env.publish_message(improvement_msg)
                    logger.info("ğŸ“¤ Ä°yileÅŸtirme talebi ve plan mesajÄ± Alex'e iletildi!")
                    
                    # Progress: DÃ¼zeltme turu baÅŸlÄ±yor
                    await self._emit_progress("Alex", "working", f"ğŸ”„ DÃ¼zeltme turu {revision_count}: Kod iyileÅŸtiriliyor...")
                    
                    # Tekrar Ã§alÄ±ÅŸtÄ±r (with timing)
                    async with AsyncTimer(f"revision_round_{revision_count}", log_on_exit=True) as rev_timer:
                        await self.team.run(n_round=n_round)
                        
                        await self._emit_progress("Alex", "completed", f"âœ… DÃ¼zeltme turu {revision_count}: Kod gÃ¼ncellendi")
                        await self._emit_progress("Bob", "working", f"ğŸ§ª DÃ¼zeltme turu {revision_count}: Testler gÃ¼ncelleniyor...")
                        
                        # Charlie'nin revision turunda da review yapmasÄ± iÃ§in ek round
                        # Manuel tetikleme hacklerini kaldÄ±rdÄ±k - sadece team.run() kullanÄ±yoruz
                        logger.debug("ğŸ” Charlie'nin revision review yapmasÄ± iÃ§in ek round Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor...")
                        await self.team.run(n_round=1)  # Charlie'nin Bob'un mesajÄ±nÄ± gÃ¶zlemlemesi ve review yapmasÄ± iÃ§in
                        
                        await self._emit_progress("Bob", "completed", f"âœ… DÃ¼zeltme turu {revision_count}: Testler gÃ¼ncellendi")
                        await self._emit_progress("Charlie", "working", f"ğŸ” DÃ¼zeltme turu {revision_count}: Tekrar inceleniyor...")
                    
                    self.phase_timings.add_phase(f"revision_round_{revision_count}", rev_timer.duration)
                    
                    # End profiling revision phase
                    if self._profiler:
                        self._profiler.end_phase()
                    
                    # Her tur sonrasÄ± hafÄ±za temizliÄŸi (offload to thread)
                    await run_in_thread(self.cleanup_memory)
                else:
                    # Review OK - dÃ¶ngÃ¼den Ã§Ä±k
                    await self._emit_progress("Charlie", "completed", "âœ… Review tamamlandÄ± - Kod onaylandÄ±!")
                    print(f"\nâœ… Review ONAYLANDI - DÃ¼zeltme gerekmiyor.")
                    break
            
            # Metrics gÃ¼ncelle - baÅŸarÄ±lÄ±
            metric.revision_rounds = revision_count
            metric.success = True
            metric.cache_hits = self._cache_hits
            metric.cache_misses = self._cache_misses
            
            # GerÃ§ek token kullanÄ±mÄ±nÄ± hesapla
            metric.token_usage = self._calculate_token_usage()
            metric.estimated_cost = budget["investment"]
            
            # Start profiling result persistence phase
            if self._profiler:
                self._profiler.start_phase("result_persistence")
            
            # Final operations: run results collection and cleanup concurrently
            async with AsyncTimer("final_operations", log_on_exit=True) as final_timer:
                # Use TaskGroup for concurrent final operations (Python 3.11+)
                # Fallback to gather for older Python versions
                try:
                    # Try Python 3.11+ TaskGroup
                    async with asyncio.TaskGroup() as tg:
                        # Collect results in thread (file I/O)
                        results_task = tg.create_task(
                            run_in_thread(self._collect_results)
                        )
                        # Final cleanup in thread
                        cleanup_task = tg.create_task(
                            run_in_thread(self.cleanup_memory)
                        )
                    
                    results = results_task.result()
                except AttributeError:
                    # Fallback for Python < 3.11: use gather
                    logger.debug("TaskGroup not available, using asyncio.gather")
                    results, _ = await asyncio.gather(
                        run_in_thread(self._collect_results),
                        run_in_thread(self.cleanup_memory)
                    )
            
            self.phase_timings.cleanup_duration = final_timer.duration
            self.phase_timings.add_phase("final_operations", final_timer.duration)
            self.phase_timings.total_duration = time.time() - start_time
            
            # End profiling result persistence phase
            if self._profiler:
                self._profiler.end_phase()
            
            # End execute profiling phase
            if self._profiler:
                self._profiler.end_phase()
            
            # KullanÄ±cÄ±ya gÃ¶rÃ¼nen bilgi _show_metrics_report ile basÄ±lÄ±yor
            logger.debug(f"GÃ¶rev tamamlandÄ± - {revision_count} dÃ¼zeltme turu yapÄ±ldÄ±")
            
            return results
            
        except Exception as e:
            # Hata durumu
            metric.success = False
            metric.error_message = str(e)
            logger.error(f"âŒ GÃ¶rev hatasÄ±: {e}")
            return f"âŒ GÃ¶rev baÅŸarÄ±sÄ±z: {e}"
            
        finally:
            # Metrics finalize (sadece metrics aktifse)
            metric.end_time = time.time()
            
            if self.metrics is not None:
                self.metrics.append(metric)
                self._show_metrics_report(metric)
    
    def _show_metrics_report(self, metric: TaskMetrics):
        """Tek bir gÃ¶revin metrik raporunu gÃ¶ster"""
        status_emoji = "âœ…" if metric.success else "âŒ"
        
        print(f"\n{'='*60}")
        print(f"ğŸ“Š GÃ–REV METRÄ°KLERÄ°")
        print(f"{'='*60}")
        print(f"ğŸ“Œ GÃ¶rev: {metric.task_name}")
        print(f"{status_emoji} Durum: {'BaÅŸarÄ±lÄ±' if metric.success else 'BaÅŸarÄ±sÄ±z'}")
        print(f"â±ï¸  SÃ¼re: {metric.duration_formatted}")
        print(f"ğŸ¯ KarmaÅŸÄ±klÄ±k: {metric.complexity}")
        print(f"ğŸ”„ DÃ¼zeltme TurlarÄ±: {metric.revision_rounds}")
        print(f"ğŸª™ Tahmini Token: ~{metric.token_usage}")
        print(f"ğŸ’° Tahmini Maliyet: ${metric.estimated_cost:.4f}")
        print(f"âš¡ Cache: hits={metric.cache_hits}, misses={metric.cache_misses}")
        if metric.error_message:
            print(f"âš ï¸  Hata: {metric.error_message}")
        print(f"{'='*60}\n")
    
    def get_all_metrics(self) -> List[dict]:
        """TÃ¼m gÃ¶rev metriklerini dÃ¶ndÃ¼r (profiling data dahil)"""
        metrics_list = []
        if self.metrics:
            metrics_list = [m.to_dict() for m in self.metrics]
        
        # Add profiling data if available
        if self._profiler:
            profiling_data = self._profiler.to_run_metrics()
            # Attach profiling to each metric or as a separate entry
            return {
                "task_metrics": metrics_list,
                "profiling": profiling_data,
            }
        
        return metrics_list
    
    def get_metrics_summary(self) -> str:
        """TÃ¼m metriklerin Ã¶zetini dÃ¶ndÃ¼r"""
        if not self.metrics:
            return "ğŸ“Š Metrikler devre dÄ±ÅŸÄ± veya henÃ¼z kaydedilmedi."
        
        total_tasks = len(self.metrics)
        successful = sum(1 for m in self.metrics if m.success)
        failed = total_tasks - successful
        total_duration = sum(m.duration_seconds for m in self.metrics)
        total_cost = sum(m.estimated_cost for m in self.metrics)
        
        summary = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                  ğŸ“Š METRÄ°K Ã–ZETÄ°                          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Œ Toplam GÃ¶rev: {total_tasks}
âœ… BaÅŸarÄ±lÄ±: {successful}
âŒ BaÅŸarÄ±sÄ±z: {failed}
â±ï¸  Toplam SÃ¼re: {total_duration:.1f}s
ğŸ’° Toplam Maliyet: ${total_cost:.4f}
"""
        return summary
    
    def get_phase_timings(self) -> dict:
        """Get phase timing data for the current task."""
        return self.phase_timings.to_dict()
    
    def show_phase_timings(self) -> str:
        """Get a formatted phase timings report."""
        return self.phase_timings.summary()
    
    def _collect_raw_results(self) -> tuple:
        """Ãœretilen kod, test ve review'larÄ± ham olarak dÃ¶ndÃ¼r
        
        Returns:
            tuple: (code, tests, review) - Her biri string
        """
        code_content = ""
        test_content = ""
        review_content = ""
        
        if hasattr(self.team, 'env') and hasattr(self.team.env, 'roles'):
            for role in self.team.env.roles.values():
                # Adapter kullanarak gÃ¼venli eriÅŸim
                mem_store = MetaGPTAdapter.get_memory_store(role)
                if mem_store is None:
                    continue
                
                # MesajlarÄ± adapter Ã¼zerinden al
                messages = MetaGPTAdapter.get_messages(mem_store)
                
                # Her role iÃ§in en son mesajÄ± al (mesajlar zaman sÄ±rasÄ±na gÃ¶re)
                for msg in messages:
                    # En son mesajlarÄ± al (sonraki mesajlar Ã¶ncekileri override eder)
                    if msg.role == "Engineer":
                        code_content = msg.content
                    elif msg.role == "Tester":
                        test_content = msg.content
                    elif msg.role == "Reviewer":
                        review_content = msg.content
        
        return code_content, test_content, review_content
    
    async def _emit_progress(self, agent_name: str, status: str, message: str):
        """Agent ilerleme durumunu callback'e bildir
        
        Args:
            agent_name: Agent adÄ± (Mike, Alex, Bob, Charlie)
            status: Durum (working, completed, error)
            message: Ä°lerleme mesajÄ±
        """
        if hasattr(self, '_progress_callback') and self._progress_callback:
            try:
                # Callback async olabilir
                import asyncio
                if asyncio.iscoroutinefunction(self._progress_callback):
                    await self._progress_callback(agent_name, status, message)
                else:
                    self._progress_callback(agent_name, status, message)
            except Exception as e:
                logger.warning(f"Progress callback failed: {e}")
        
        # Console'a da yazdÄ±r
        status_emoji = "ğŸ”„" if status == "working" else "âœ…" if status == "completed" else "âŒ"
        print(f"{status_emoji} {agent_name}: {message}")
        logger.debug(f"Agent progress: {agent_name} - {status} - {message}")
    
    def _collect_results(self) -> str:
        """Ãœretilen kod, test ve review'larÄ± topla ve kaydet"""
        code_content, test_content, review_content = self._collect_raw_results()
        
        # SonuÃ§larÄ± kaydet
        self._save_results(code_content, test_content, review_content)
        
        # Ã–zet sonuÃ§
        summary = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    ğŸ“Š SONUÃ‡ Ã–ZETÄ°                         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ’» Alex (Engineer): {'âœ… Kod yazÄ±ldÄ±' if code_content else 'âŒ Kod yok'}
ğŸ§ª Bob (Tester): {'âœ… Testler yazÄ±ldÄ±' if test_content else 'âŒ Test yok'}
ğŸ” Charlie (Reviewer): {'âœ… Review tamamlandÄ±' if review_content else 'âŒ Review yok'}

ğŸ“ Dosyalar output/ dizinine kaydedildi.
"""
        return summary
    
    def _safe_write_file(self, path: str, content: str):
        """
        DosyayÄ± gÃ¼venli ÅŸekilde yaz:
        - KlasÃ¶rÃ¼ oluÅŸtur
        - Dosya zaten varsa .bak_yedek al
        - Sonra yeni iÃ§eriÄŸi yaz
        """
        import shutil
        
        os.makedirs(os.path.dirname(path), exist_ok=True)
        
        if os.path.exists(path):
            ts = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_path = f"{path}.bak_{ts}"
            shutil.copy2(path, backup_path)
            logger.info(f"ğŸ§¯ Yedek alÄ±ndÄ±: {backup_path}")
        
        with open(path, "w", encoding="utf-8") as f:
            f.write(content)
        
        logger.info(f"ğŸ’¾ Dosya yazÄ±ldÄ±: {path}")
    
    def _save_results(self, code: str, tests: str, review: str):
        """Ãœretilen kodu, testleri ve review'Ä± dosyalara kaydet - HTML/CSS/JS dahil"""
        # re ve datetime zaten en Ã¼stte import edilmiÅŸ
        
        # Output dizini oluÅŸtur
        if not self.output_dir_base:
            return

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = f"{self.output_dir_base}/mgx_team_{timestamp}"
        os.makedirs(output_dir, exist_ok=True)
        
        saved_files = []
        
        # Kod dosyalarÄ±nÄ± kaydet - tÃ¼m dilleri destekle
        if code:
            # HTML dosyalarÄ±nÄ± Ã§Ä±kar ve kaydet
            html_blocks = re.findall(r'```html\s*(.*?)\s*```', code, re.DOTALL | re.IGNORECASE)
            for i, block in enumerate(html_blocks):
                if block.strip():
                    filename = "index.html" if i == 0 else f"page_{i}.html"
                    html_path = f"{output_dir}/{filename}"
                    self._safe_write_file(html_path, block.strip())
                    saved_files.append(filename)
            
            # CSS dosyalarÄ±nÄ± Ã§Ä±kar ve kaydet
            css_blocks = re.findall(r'```css\s*(.*?)\s*```', code, re.DOTALL | re.IGNORECASE)
            for i, block in enumerate(css_blocks):
                if block.strip():
                    filename = "style.css" if i == 0 else f"style_{i}.css"
                    css_path = f"{output_dir}/{filename}"
                    self._safe_write_file(css_path, block.strip())
                    saved_files.append(filename)
            
            # JavaScript dosyalarÄ±nÄ± Ã§Ä±kar ve kaydet
            js_blocks = re.findall(r'```(?:javascript|js)\s*(.*?)\s*```', code, re.DOTALL | re.IGNORECASE)
            for i, block in enumerate(js_blocks):
                if block.strip():
                    filename = "script.js" if i == 0 else f"script_{i}.js"
                    js_path = f"{output_dir}/{filename}"
                    self._safe_write_file(js_path, block.strip())
                    saved_files.append(filename)
            
            # PHP dosyalarÄ±nÄ± Ã§Ä±kar ve kaydet
            php_blocks = re.findall(r'```php\s*(.*?)\s*```', code, re.DOTALL | re.IGNORECASE)
            for i, block in enumerate(php_blocks):
                if block.strip():
                    filename = "index.php" if i == 0 else f"page_{i}.php"
                    php_path = f"{output_dir}/{filename}"
                    self._safe_write_file(php_path, block.strip())
                    saved_files.append(filename)
            
            # Python kod bloklarÄ±nÄ± Ã§Ä±kar (farklÄ± formatlarÄ± destekle)
            python_blocks = re.findall(r'```python\s*(.*?)\s*```', code, re.DOTALL | re.IGNORECASE)
            # EÄŸer spesifik dil belirtilmemiÅŸ bloklar varsa onlarÄ± da al
            generic_blocks = re.findall(r'```\s*\n(.*?)\s*```', code, re.DOTALL)
            
            all_python_blocks = python_blocks + [b for b in generic_blocks if 'def ' in b or 'import ' in b or 'class ' in b]
            
            if all_python_blocks:
                main_py_path = f"{output_dir}/main.py"
                main_py_content = "# MGX Style Team tarafÄ±ndan Ã¼retildi\n"
                main_py_content += f"# Tarih: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
                
                for block in all_python_blocks:
                    if block.strip():
                        main_py_content += block.strip() + "\n\n"
                
                self._safe_write_file(main_py_path, main_py_content)
                saved_files.append("main.py")
            elif not html_blocks and not css_blocks and not js_blocks and not php_blocks:
                # HiÃ§bir kod bloÄŸu bulunamazsa ham iÃ§eriÄŸi Python olarak kaydet
                main_py_path = f"{output_dir}/main.py"
                main_py_content = "# MGX Style Team tarafÄ±ndan Ã¼retildi\n"
                main_py_content += f"# Tarih: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
                main_py_content += code
                self._safe_write_file(main_py_path, main_py_content)
                saved_files.append("main.py")
        
        # Test dosyasÄ±nÄ± kaydet
        if tests:
            test_blocks = re.findall(r'```(?:python)?\s*(.*?)\s*```', tests, re.DOTALL)
            
            test_py_path = f"{output_dir}/test_main.py"
            test_py_content = "# MGX Style Team tarafÄ±ndan Ã¼retildi - TEST DOSYASI\n"
            test_py_content += f"# Tarih: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
            
            if test_blocks:
                for block in test_blocks:
                    if block.strip():
                        test_py_content += block.strip() + "\n\n"
            else:
                test_py_content += tests
            
            self._safe_write_file(test_py_path, test_py_content)
            saved_files.append("test_main.py")
        
        # Review dosyasÄ±nÄ± kaydet
        if review:
            review_path = f"{output_dir}/review.md"
            review_content = "# Kod Ä°nceleme Raporu\n\n"
            review_content += f"**Tarih:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
            review_content += review
            
            self._safe_write_file(review_path, review_content)
            saved_files.append("review.md")
        
        logger.info(f"ğŸ“ TÃ¼m dosyalar kaydedildi ({len(saved_files)} dosya): {output_dir}/")
    
    def get_progress(self) -> str:
        """Ä°lerleme durumunu al"""
        if not self.progress:
            return "ğŸ“Š HenÃ¼z ilerleme kaydedilmedi."
        
        return "\n".join([f"âœ… {p}" for p in self.progress])
    
    # ============================================
    # INCREMENTAL DEVELOPMENT - ArtÄ±mlÄ± GeliÅŸtirme
    # ============================================
    
    async def run_incremental(self, requirement: str, project_path: str = None, 
                               fix_bug: bool = False, ask_confirmation: bool = True) -> str:
        """
        Mevcut projeye yeni Ã¶zellik ekle veya bug dÃ¼zelt
        
        Args:
            requirement: Yeni gereksinim veya bug aÃ§Ä±klamasÄ±
            project_path: Mevcut proje yolu (None ise yeni proje)
            fix_bug: True ise bug dÃ¼zeltme modu
            ask_confirmation: True ise plan onayÄ± iÃ§in kullanÄ±cÄ±dan input bekler
                             False ise sessiz modda otomatik onaylar (non-interactive)
        
        Returns:
            SonuÃ§ Ã¶zeti
        """
        
        mode = "ğŸ› BUG DÃœZELTME" if fix_bug else "â• YENÄ° Ã–ZELLÄ°K"
        
        # KullanÄ±cÄ±ya gÃ¶rÃ¼nen bilgi incremental_main fonksiyonunda print ile basÄ±lÄ±yor
        logger.debug(f"{mode} modu baÅŸlatÄ±lÄ±yor")
        
        if project_path:
            logger.debug(f"Proje yolu: {project_path}")
            
            # Proje yapÄ±sÄ±nÄ± kontrol et
            if os.path.exists(project_path):
                docs_path = os.path.join(project_path, "docs")
                src_path = os.path.join(project_path, "src")
                
                # Mevcut dosyalarÄ± oku
                existing_files = []
                if os.path.exists(src_path):
                    for f in os.listdir(src_path):
                        if f.endswith('.py'):
                            existing_files.append(f)
                
                logger.info(f"ğŸ“„ Mevcut dosyalar: {existing_files}")
                
                # Mevcut kodu hafÄ±zaya ekle
                self.add_to_memory("System", "ProjectContext", f"Proje: {project_path}, Dosyalar: {existing_files}")
        
        # Analiz et
        logger.info(f"\nğŸ“¨ Ä°stek: {requirement}")
        
        if fix_bug:
            # Bug dÃ¼zeltme analizi
            analysis_prompt = f"""[INCREMENTAL - BUG DÃœZELTME]

Hata: {requirement}

LÃ¼tfen:
1. HatanÄ±n olasÄ± nedenini belirle
2. DÃ¼zeltme planÄ± oluÅŸtur
3. Etkilenecek dosyalarÄ± listele
"""
        else:
            # Yeni Ã¶zellik analizi
            analysis_prompt = f"""[INCREMENTAL - YENÄ° Ã–ZELLÄ°K]

Ä°stek: {requirement}

LÃ¼tfen:
1. Ã–zelliÄŸin karmaÅŸÄ±klÄ±ÄŸÄ±nÄ± deÄŸerlendir (XS/S/M/L/XL)
2. Mevcut koda etkisini analiz et
3. Uygulama planÄ± oluÅŸtur
"""
        
        # Mike'a analiz yaptÄ±r (analysis_prompt dahil)
        analysis = await self.analyze_and_plan(analysis_prompt)
        
        # Plan onayÄ±
        print(f"\n{'='*50}")
        print(f"ğŸ“‹ {mode} PLANI:")
        print(f"{'='*50}")
        print(analysis)
        
        if ask_confirmation:
            # Interactive mod - kullanÄ±cÄ±dan onay bekle
            print(f"\nâš ï¸ Devam etmek iÃ§in ENTER'a basÄ±n (iptal iÃ§in 'q'):")
            user_input = input()
            if user_input.lower() == 'q':
                return "âŒ Ä°ÅŸlem iptal edildi."
        else:
            # Non-interactive / sessiz mod - otomatik onayla
            logger.info("ğŸ¤– Sessiz mod: Plan otomatik onaylandÄ±")
        
        self.approve_plan()
        
        # DeÄŸiÅŸiklikleri uygula
        result = await self.execute(n_round=3)
        
        # SonuÃ§
        summary = f"""
        {'='*50}
        âœ… {mode} TAMAMLANDI!
        {'='*50}
        
        ğŸ“ Ä°stek: {requirement}
        ğŸ“ Proje: {project_path or 'Yeni Proje'}
        
        ğŸ“‹ YapÄ±lan DeÄŸiÅŸiklikler:
        {result[:500]}...
        
        ğŸ’¾ HafÄ±za gÃ¼ncellendi.
        """
        
        logger.info(summary)
        return summary
    
    async def add_feature(self, feature: str, project_path: str) -> str:
        """Mevcut projeye yeni Ã¶zellik ekle"""
        return await self.run_incremental(feature, project_path, fix_bug=False)
    
    async def fix_bug(self, bug_description: str, project_path: str) -> str:
        """Mevcut projedeki bug'Ä± dÃ¼zelt"""
        return await self.run_incremental(bug_description, project_path, fix_bug=True)
    
    def list_project_files(self, project_path: str) -> list:
        """Proje dosyalarÄ±nÄ± listele"""
        
        files = []
        for root, dirs, filenames in os.walk(project_path):
            # .git ve __pycache__ gibi klasÃ¶rleri atla
            dirs[:] = [d for d in dirs if d not in ['.git', '__pycache__', 'node_modules', '.venv']]
            
            for f in filenames:
                if f.endswith(('.py', '.js', '.ts', '.html', '.css', '.json', '.yaml', '.yml')):
                    rel_path = os.path.relpath(os.path.join(root, f), project_path)
                    files.append(rel_path)
        
        return files
    
    def get_project_summary(self, project_path: str) -> str:
        """Proje Ã¶zetini al"""
        
        files = self.list_project_files(project_path)
        
        summary = f"""
        ğŸ“ PROJE Ã–ZETÄ°: {project_path}
        {'='*40}
        
        ğŸ“„ Dosya SayÄ±sÄ±: {len(files)}
        
        ğŸ“‚ Dosyalar:
        """
        
        for f in files[:20]:  # Ä°lk 20 dosya
            summary += f"\n        - {f}"
        
        if len(files) > 20:
            summary += f"\n        ... ve {len(files) - 20} dosya daha"
        
        return summary


# ============================================
# KULLANIM Ã–RNEÄÄ°
# ============================================
async def main(human_reviewer: bool = False, custom_task: str = None):
    """
    MGX tarzÄ± takÄ±m Ã¶rneÄŸi
    
    Args:
        human_reviewer: True ise Charlie (Reviewer) insan olarak Ã§alÄ±ÅŸÄ±r
        custom_task: Ã–zel gÃ¶rev tanÄ±mÄ± (None ise varsayÄ±lan gÃ¶rev)
    """
    
    mode_text = "ğŸ§‘ Ä°NSAN MODU" if human_reviewer else "ğŸ¤– LLM MODU"
    
    print(f"""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘           MGX STYLE MULTI-AGENT TEAM                     â•‘
    â•‘                                                          â•‘
    â•‘  ğŸ‘¤ Mike (Team Leader) - GÃ¶rev analizi ve planlama       â•‘
    â•‘  ğŸ‘¤ Alex (Engineer) - Kod yazma                          â•‘
    â•‘  ğŸ‘¤ Bob (Tester) - Test yazma                            â•‘
    â•‘  ğŸ‘¤ Charlie (Reviewer) - Kod inceleme [{mode_text}]      â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # TakÄ±mÄ± oluÅŸtur (human_reviewer=True yaparak insan olarak katÄ±labilirsin)
    mgx_team = MGXStyleTeam(human_reviewer=human_reviewer)
    
    # GÃ¶rev tanÄ±mla (varsayÄ±lan veya Ã¶zel)
    task = custom_task or "Listedeki sayÄ±larÄ±n Ã§arpÄ±mÄ±nÄ± hesaplayan bir Python fonksiyonu yaz"
    
    # 1. Analiz ve Plan (stream ile canlÄ± gÃ¶sterilir)
    print("\nğŸ“‹ ADIM 1: GÃ¶rev Analizi ve Plan OluÅŸturma")
    print("-" * 50)
    await mgx_team.analyze_and_plan(task)
    # Stream ile canlÄ± gÃ¶sterildi, tekrar print etmeye gerek yok
    
    # 2. Plan OnayÄ± (gerÃ§ek uygulamada kullanÄ±cÄ±dan alÄ±nÄ±r)
    print("\nâœ… ADIM 2: Plan OnayÄ±")
    print("-" * 50)
    mgx_team.approve_plan()
    
    # 3. GÃ¶rev YÃ¼rÃ¼tme (her agent canlÄ± Ã§Ä±ktÄ± verir)
    print("\nğŸš€ ADIM 3: GÃ¶rev YÃ¼rÃ¼tme")
    print("-" * 50)
    await mgx_team.execute()  # KarmaÅŸÄ±klÄ±ÄŸa gÃ¶re otomatik ayarlanÄ±r
    # Agent'larÄ±n Ã§Ä±ktÄ±larÄ± stream ile canlÄ± gÃ¶sterildi
    
    # 4. HafÄ±za GÃ¼nlÃ¼ÄŸÃ¼
    print("\nğŸ“‹ ADIM 4: HafÄ±za GÃ¼nlÃ¼ÄŸÃ¼")
    print("-" * 50)
    print(mgx_team.show_memory_log())
    
    # 5. Ä°lerleme Durumu
    print("\nğŸ“Š ADIM 5: Ä°lerleme Durumu")
    print("-" * 50)
    print(mgx_team.get_progress())
    
    print("\n" + "=" * 50)
    print("ğŸŠ MGX Style TakÄ±m Ã§alÄ±ÅŸmasÄ± tamamlandÄ±!")
    print("=" * 50)


def cli_main():
    """CLI entry point - parses arguments and runs appropriate mode."""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="MGX Style Multi-Agent Team",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Ã–rnekler:
  # Normal mod (yeni gÃ¶rev)
  python mgx_style_team.py
  
  # Ä°nsan reviewer modu
  python mgx_style_team.py --human
  
  # Yeni Ã¶zellik ekle (mevcut projeye)
  python mgx_style_team.py --add-feature "Add login system" --project-path "./my_project"
  
  # Bug dÃ¼zelt
  python mgx_style_team.py --fix-bug "TypeError: x is not defined" --project-path "./my_project"
  
  # Ã–zel gÃ¶rev
  python mgx_style_team.py --task "Fibonacci hesaplayan fonksiyon yaz"
        """
    )
    
    parser.add_argument(
        "--human", 
        action="store_true", 
        help="Charlie (Reviewer) iÃ§in insan modu aktif et"
    )
    
    parser.add_argument(
        "--task",
        type=str,
        default=None,
        help="Ã–zel gÃ¶rev tanÄ±mÄ±"
    )
    
    parser.add_argument(
        "--project-path",
        type=str,
        default=None,
        help="Mevcut proje yolu (incremental development iÃ§in)"
    )
    
    parser.add_argument(
        "--add-feature",
        type=str,
        default=None,
        help="Mevcut projeye yeni Ã¶zellik ekle"
    )
    
    parser.add_argument(
        "--fix-bug",
        type=str,
        default=None,
        help="Mevcut projedeki bug'Ä± dÃ¼zelt"
    )
    
    parser.add_argument(
        "--no-confirm",
        action="store_true",
        help="Plan onayÄ± bekleme (sessiz mod)"
    )
    
    args = parser.parse_args()
    
    # Incremental Development modlarÄ±
    if args.add_feature:
        print("\nâ• YENÄ° Ã–ZELLÄ°K EKLEME MODU")
        asyncio.run(incremental_main(args.add_feature, args.project_path, fix_bug=False, ask_confirmation=not args.no_confirm))
    
    elif args.fix_bug:
        print("\nğŸ› BUG DÃœZELTME MODU")
        asyncio.run(incremental_main(args.fix_bug, args.project_path, fix_bug=True, ask_confirmation=not args.no_confirm))
    
    # Normal mod
    else:
        if args.human:
            print("\nğŸ§‘ Ä°NSAN MODU AKTÄ°F: Charlie olarak siz review yapacaksÄ±nÄ±z!")
            print("   SÄ±ra size geldiÄŸinde terminal'den input beklenir.\n")
        
        if args.task:
            print(f"\nğŸ“ Ã–ZEL GÃ–REV: {args.task}\n")
        
        asyncio.run(main(human_reviewer=args.human, custom_task=args.task))


async def incremental_main(requirement: str, project_path: str = None, fix_bug: bool = False, ask_confirmation: bool = True):
    """
    ArtÄ±mlÄ± geliÅŸtirme modu
    
    Args:
        requirement: Yeni gereksinim veya bug aÃ§Ä±klamasÄ±
        project_path: Mevcut proje yolu
        fix_bug: True ise bug dÃ¼zeltme modu
        ask_confirmation: True ise plan onayÄ± bekle (sessiz mod iÃ§in False)
    """
    mode = "ğŸ› BUG DÃœZELTME" if fix_bug else "â• YENÄ° Ã–ZELLÄ°K"
    
    print(f"""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘        MGX STYLE - INCREMENTAL DEVELOPMENT               â•‘
    â•‘                                                          â•‘
    â•‘  {mode:^52} â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    mgx_team = MGXStyleTeam(human_reviewer=False)
    
    if project_path:
        print(f"\nğŸ“ Proje: {project_path}")
        print(mgx_team.get_project_summary(project_path))
    
    result = await mgx_team.run_incremental(requirement, project_path, fix_bug, ask_confirmation)
    print(result)


if __name__ == "__main__":
    cli_main()